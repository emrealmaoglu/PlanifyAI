

A Technical Guide: Applying Deep Reinforcement Learning to Sequential Spatial Planning and Optimization


1. Introduction: The DRL Framework for Generative Spatial Planning

Sequential spatial planning—such as the placement of buildings on a campus, facilities in a factory, or components on a microchip—is a generative design task of immense combinatorial complexity.1 Unlike traditional optimization methods that seek a single optimal solution from a fixed search space, this problem requires a sequential decision-making process. Each decision (e.g., placing building $k$) constrains all subsequent decisions, creating complex, long-term dependencies. This process is naturally modeled as a Markov Decision Process (MDP) 3, making it a canonical fit for Deep Reinforcement Learning (DRL).6
Two core challenges define this problem domain:
Combinatorial Complexity: The state and action spaces are exceptionally large. For a 100x100 grid and 7 building types, a naive discrete action space contains $100 \times 100 \times 7 = 70,000$ possible actions at each step. For a sequence of $N=50$ buildings, the total number of possible layouts is astronomical. Standard value-based DRL algorithms, which must estimate a value for every action, become intractable.7
Sparse and Delayed Rewards: The quality of a layout is defined by multi-objective criteria (e.g., energy efficiency, adjacency requirements, workflow cost) that can often only be evaluated after the entire layout is complete.9 An agent placing building #1 receives no immediate feedback on its contribution to the final layout's quality. This sparse reward signal 10 makes the credit assignment problem nearly impossible to solve with standard RL.
DRL offers a unique solution paradigm. Instead of relying on brittle heuristics or exhaustive search, DRL allows an agent (the "planner") to learn a generative policy, denoted $\pi(a|s)$. This policy is a stochastic function that, given the current partial layout (state $s$), outputs a probability distribution over a set of intelligent placement (actions $a$). The agent learns this policy by interacting with the environment, navigating the trade-offs between 5-8 competing objectives to maximize a cumulative reward signal.
This report provides a comprehensive technical guide to formulating, designing, and implementing a SOTA DRL framework for sequential spatial planning. It covers the complete workflow, from the formal mathematical theory of the MDP (Section 2) and advanced reward engineering (Section 5) to state-of-the-art neural network architectures (Section 4) and practical implementation templates (Section 10).

2. Problem Formulation: Building Placement as a Markov Decision Process (MDP)

To apply DRL, the sequential placement problem must first be formally defined as a Markov Decision Process (MDP), $M = (S, A, P, R, \gamma)$, where $\gamma$ is the discount factor.3 For this problem, we define a finite-horizon, deterministic MDP.

2.1 State Space ($S$)

The state $s_t \in S$ at timestep $t$ must fully capture the partial layout and the context for the next decision. A simple occupancy grid is insufficient for complex relational goals. Therefore, a hybrid (composite) state representation is required:
$s_t = \{ \mathbf{G}_t, \mathbf{V}_t, k_t \}$
$\mathbf{G}_t$: 2D Spatial Grid (Perception)
This is a $C \times H \times W$ tensor (e.g., $C=8$, $H=100$, $W=100$) representing the spatial canvas. Convolutional Neural Networks (CNNs) are highly effective at processing this data due to their translational equivariance.12
The $C$ channels encode all necessary spatial information:
Occupancy: Binary mask (1=occupied, 0=free).
Building Type: Integer ID (0-7) of the building at each cell.
Constraint Zones: Masks for "no-build" areas, "residential-only" zones, etc.
Proximity Maps: Pre-computed channels (e.g., distance to nearest road, distance to power grid).
... (Other channels as required by objectives).
$\mathbf{V}_t$: Relational Graph (Reasoning)
This is a graph $G_t = (V_t, E_t)$ processed by a Graph Neural Network (GNN). CNNs excel at local patterns but fail at relational reasoning (e.g., "Building A must have a line of sight to Building C," or "minimize adjacency between labs and residential").15
$V_t$ is the set of $t$ buildings already placed, with node features (e.g., building type, size).
$E_t$ represents adjacency, utility connections, or other relationships.17
$k_t$: Context Vector (Focus)
This is a vector encoding the features (type, size, requirements) of the next building $(t+1)$ to be placed. This context is essential for the agent to make a placement decision for that specific building.

2.2 Action Space ($A$)

The design of the action space is the most critical decision for tractability.
Option 1: Large Discrete Space (Naive)
$A$ is a single integer, $a \in \{0,..., 69,999\}$, representing a (Grid Cell $x$, Grid Cell $y$, Building Type) tuple.
Challenge: This space is intractably large for value-based methods (like DQN) that must compute a Q-value for each action.7 It also ignores the inherent structure of the space; placing at $(50, 50)$ is unrelated to placing at $(50, 51)$.
Option 2: Continuous Space (SOTA Proposal)
$A$ is a low-dimensional continuous vector $a \in \mathbb{R}^d$. For $d=9$, $a = (x, y, \text{type}_1,..., \text{type}_7)$.
$x, y \in [-1, 1]$ are scaled coordinates on the grid.
$(\text{type}_1,..., \text{type}_7)$ are logits, and the building type is selected via $\text{argmax}$.
Justification: This formulation leverages the insight that similar actions (e.g., nearby coordinates) should have similar outcomes.8 It enables the use of highly sample-efficient, powerful algorithms like Soft Actor-Critic (SAC).18
Option 3: Hybrid/Parameterized Space
This space explicitly combines continuous and discrete parameters $a_t = (a_c, a_d)$, where $a_c = (x, y)$ and $a_d = \text{type}$.19 This is an advanced but effective alternative.
For the remainder of this guide, we assume Option 2 (Continuous) for SOTA algorithms (SAC) and Option 1 (Discrete) for baseline comparisons (PPO).

2.3 Transition Dynamics ($P(s'|s, a)$)

The transition function $P: S \times A \to S$ is deterministic. Given a state $s_t$ (partial layout) and an action $a_t$ (place building $k_t$ at $(x, y)$), the next state $s_{t+1}$ is uniquely determined. The grid $\mathbf{G}_{t+1}$ is updated, the graph $\mathbf{V}_{t+1}$ adds a node, and the context $k_{t+1}$ advances to the next building in the queue. This determinism is ideal for off-policy algorithms.

2.4 Reward Function ($R(s, a)$)

The reward function $R$ is defined by the 5-8 competing objectives. This presents two problems:
Multi-Objective: The reward is a vector $R =$.
Delayed: Most objectives $R_i$ can only be calculated at the terminal state $s_N$.9
This delayed reward problem is solved using Reward Shaping, which is detailed in Section 5. The immediate reward $r_t$ at each step will be engineered to be a dense, informative signal.

2.5 Episode Termination

An episode $E = (s_0, a_0, r_1, s_1,..., s_N)$ terminates when either:
All $N$ buildings have been placed ($t=N$).
A hard constraint is violated (e.g., placing a building out-of-bounds or on an existing building), resulting in a large negative reward.

3. DRL Algorithms for Spatial Optimization

The choice of DRL algorithm is dictated by the action space formulation. The primary distinction is between on-policy methods, which are stable but sample-inefficient, and off-policy methods, which are more efficient but complex.

3.1 Value-Based Methods (e.g., DQN)

Algorithm: Deep Q-Networks (DQN), Double DQN, Dueling DQN.23
Mechanism: Learns a Q-function $Q(s, a)$ that estimates the expected future return of taking action $a$ in state $s$. The policy is $\pi(s) = \text{argmax}_a Q(s, a)$.
Applicability: Not viable for this problem. To find the $\text{argmax}$, the network must output 70,000 Q-values, one for each discrete action. This is computationally prohibitive and suffers from catastrophic exploration challenges, as the agent can never sample all actions.7

3.2 Policy Gradient (PG) Methods (e.g., PPO)

Algorithm: Proximal Policy Optimization (PPO) is the SOTA on-policy algorithm.24
Mechanism: Directly learns a stochastic policy $\pi_\theta(a|s)$. The network outputs a probability distribution (e.g., a softmax) over the 70,000 discrete actions. It uses a "clipped" surrogate objective function to ensure stable policy updates.26
Applicability: PPO can handle the large discrete action space. It does not need to evaluate all actions, only to sample from its learned distribution. However, it is on-policy, meaning it must discard all collected $(s, a, r, s')$ transitions after a single gradient update. This leads to very low sample efficiency, requiring millions of episodes to converge.

3.3 Actor-Critic (Off-Policy) Methods (e.g., SAC)

Algorithm: Soft Actor-Critic (SAC) 27 and its variants like Twin Delayed DDPG (TD3) 28 are the SOTA for continuous control.18
Mechanism: This is the recommended approach, contingent on using the continuous action space from Section 2.2.
Actor (Policy) $\pi_\theta(s)$: A network that outputs the parameters (mean $\mu$ and variance $\sigma$) of a Tanh-Gaussian distribution for the continuous action vector $a$.
Critic (Value) $Q_\phi(s, a)$: A network that learns the value of the continuous state-action pair. SAC uses a clipped double-Q network (like TD3) to prevent Q-value overestimation.30
Entropy Regularization: SAC's objective is unique: it maximizes reward plus policy entropy: $J(\theta) = \mathbb{E}[\sum_t r_t + \alpha \mathcal{H}(\pi(\cdot|s_t))]$. This $\alpha$-weighted entropy term $\mathcal{H}$ is a powerful, built-in exploration mechanism. It explicitly encourages the policy to be as random as possible while still maximizing reward, preventing policy collapse to a single "safe" placement and driving the discovery of diverse, creative, and high-quality layouts.27
Applicability: SAC is off-policy. It stores all past transitions in a large Replay Buffer and reuses them for many gradient updates. Given that our transitions are deterministic, this re-use is highly effective. SAC is far more sample-efficient than PPO 31 and is the ideal choice.

3.4 Algorithm Pseudocode

The following algorithms represent the two primary viable paths.

3.4.1 Proximal Policy Optimization (PPO-Clip)

PPO is an on-policy, actor-critic algorithm. It collects a batch of experiences and performs multiple gradient updates on that batch using a clipped objective.
$$\text{Algorithm 1: PPO-Clip (for Discrete Action Space)}$$
$$\begin{array}{l} \hline \text{Input: initial policy parameters } \theta_0, \text{ initial value function parameters } \phi_0 \\ \text{for } k = 0, 1, 2,... \text{ do} \\ \quad \text{1. Collect set of trajectories } \mathcal{D}_k = \{ \tau_i \} \text{ by running policy } \pi_{\theta_k} \text{ in the environment.} \\ \quad \text{2. Compute rewards-to-go } \hat{R}_t = \sum_{t'=t}^T \gamma^{t'-t} r_{t'} \\ \quad \text{3. Compute advantage estimates } \hat{A}_t = \hat{R}_t - V_{\phi_k}(s_t) \text{ based on the current value function } V_{\phi_k}. \\ \quad \text{4. Let } r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_k}(a_t|s_t)} \text{ be the probability ratio.} \\ \quad \text{5. Define the objective: } L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t) \right] \\ \quad \text{6. Update the policy by maximizing } L^{CLIP}(\theta) \text{ via stochastic gradient ascent (e.g., } M \text{ epochs of Adam):} \\ \qquad \theta_{k+1} = \text{Adam}(\nabla_\theta L^{CLIP}(\theta_k)) \\ \quad \text{7. Fit the value function by regression on mean-squared error:} \\ \qquad \phi_{k+1} = \text{Adam}(\nabla_\phi \frac{1}{|\mathcal{D}_k|T} \sum_{\tau \in \mathcal{D}_k} \sum_t (V_{\phi}(s_t) - \hat{R}_t)^2) \\ \text{end for} \\ \hline \end{array}$$
(Adapted from 26)

3.4.2 Soft Actor-Critic (SAC)

SAC is an off-policy, maximum-entropy algorithm. It uses a replay buffer and trains two Q-networks (for stability) and a stochastic policy network.
$$\text{Algorithm 2: Soft Actor-Critic (SAC) (for Continuous Action Space)}$$
$$\begin{array}{l} \hline \text{Input: initial policy parameters } \theta, \text{ Q-function parameters } \phi_1, \phi_2. \\ \text{Input: initial target Q-function weights } \phi_{targ, 1} \leftarrow \phi_1, \phi_{targ, 2} \leftarrow \phi_2. \\ \text{Initialize replay buffer } \mathcal{D}. \\ \text{for each iteration do} \\ \quad \text{1. Collect experience: for each environment step do} \\ \qquad a_t \sim \pi_\theta(\cdot|s_t) \\ \qquad s_{t+1} \sim P(\cdot|s_t, a_t) \\ \qquad r_t \leftarrow \text{Reward (from PBRS, see Sec 5)} \\ \qquad \mathcal{D} \leftarrow \mathcal{D} \cup \{(s_t, a_t, r_t, s_{t+1})\} \\ \quad \text{end for} \\ \quad \text{2. Perform gradient updates: for } j = 1,..., M \text{ do} \\ \qquad \text{Sample a minibatch } B = \{(s, a, r, s')\} \text{ from } \mathcal{D}. \\ \qquad \text{// Compute Q-targets with entropy} \\ \qquad a' \sim \pi_\theta(\cdot|s') \\ \qquad y(r, s') = r + \gamma \left( \min_{i=1,2} Q_{\phi_{targ, i}}(s', a') - \alpha \log \pi_\theta(a'|s') \right) \\ \qquad \text{// Update Q-functions (Critics)} \\ \qquad L_Q(\phi_i) = \mathbb{E}_{(s,a) \in B} \left[ (Q_{\phi_i}(s, a) - y(r, s'))^2 \right] \text{ for } i=1,2 \\ \qquad \phi_i \leftarrow \phi_i - \lambda_Q \nabla_{\phi_i} L_Q(\phi_i) \\ \qquad \text{// Update Policy (Actor)} \\ \qquad a_{new} \sim \pi_\theta(\cdot|s) \\ \qquad L_\pi(\theta) = \mathbb{E}_{s \in B} \left[ \min_{i=1,2} Q_{\phi_i}(s, a_{new}) - \alpha \log \pi_\theta(a_{new}|s) \right] \\ \qquad \theta \leftarrow \theta - \lambda_\pi \nabla_{\theta} L_\pi(\theta) \\ \qquad \text{// (Optional: Update entropy temperature } \alpha \text{)} \\ \qquad \text{// Update target networks (soft update)} \\ \qquad \phi_{targ, i} \leftarrow \rho \phi_{targ, i} + (1-\rho) \phi_i \text{ for } i=1,2 \\ \quad \text{end for} \\ \text{end for} \\ \hline \end{array}$$
(Adapted from 27)

4. Neural Network Architectures for Spatial State Processing

The DRL algorithm relies on a neural network as a function approximator. Given our hybrid state $s_t = \{ \mathbf{G}_t, \mathbf{V}_t, k_t \}$, a hybrid network architecture is required to effectively parse this information.32
Figure 1: Proposed Hybrid Network Architecture for SAC Actor and Critic



          STATE (s_t)          / | \         / | \    (Grid)   (Graph)  (Context)      G_t       V_t       k_t| | |    +-----+   +-----+   +-----+| CNN | | GNN | | MLP | (Encoders)    +-----+   +-----+   +-----+| | |      f_cnn     f_gnn     f_mlp       \ | /        \ | /      [ f_cnn, f_gnn, f_mlp ]   (Concatenation)|          +--------------+| FUSION HEAD || (e.g. Attn) |          +--------------+|             f_fused            /       \           /         \    +----------+    +------------------+| ACTOR | | CRITIC || (Policy) | | (Q-Function) |    +----------+    +------------------+| |     a ~ N(u,s)     Q(s,a) <-- (concat with action 'a')

4.1 Component 1: CNN for Spatial Perception

Input: The 2D grid tensor $\mathbf{G}_t$ ($C \times H \times W$).
Architecture: A stack of 2D convolutional layers, such as a ResNet backbone.13 This network excels at learning hierarchical spatial features—edges, corners, open areas, and local patterns—from the occupancy grid.34
Output: A flattened feature vector $f_{cnn}$.

4.2 Component 2: GNN for Relational Reasoning

Input: The relational graph $\mathbf{V}_t$ ($G_t = (V_t, E_t)$).
Architecture: A Graph Neural Network (GNN), such as a Graph Attention Network (GAT).36
Justification: As buildings are placed, their relationships (adjacencies, utility flows, proximity constraints) form a graph. A GNN is required to perform relational reasoning by passing messages between building nodes, capturing complex, non-local dependencies that are invisible to the CNN.16
Output: A graph-level feature vector $f_{gnn}$ (e.g., via global mean pooling).

4.3 Component 3: MLP for Context

Input: The $k_t$ vector (features of the next building).
Architecture: A simple Multi-Layer Perceptron (MLP).
Output: An embedding $f_{mlp}$.

4.4 Component 4: Attention-Based Fusion Head

Input: The concatenated feature vector $[f_{cnn}, f_{gnn}, f_{mlp}]$.
Architecture: An attention mechanism 38 or a Spatial Pyramid Pooling (SPP) layer 41 can be used.
Justification: The agent must dynamically weigh the importance of different features. When placing a "lab" (which may have strong adjacency constraints), the GNN's output $f_{gnn}$ is most important. When placing a "dormitory" (which may just need open space), the CNN's output $f_{cnn}$ is more relevant. An attention head learns to modulate these feature streams.
Output: A single, fused feature vector $f_{fused}$.
This $f_{fused}$ vector then serves as the input for the final DRL networks:
SAC Actor: $f_{fused}$ is passed through an MLP to output the $\mu$ and $\sigma$ for the continuous action distribution.
SAC Critic: $f_{fused}$ is concatenated with the action vector $a_t$ and passed through an MLP to output the scalar Q-value.

5. Guidance and Reward Engineering

The primary challenge of sparse rewards 9 can be solved by reformulating the reward function to provide a dense, immediate signal at every step.

5.1 The Sparse Reward Problem

If an agent only receives a reward $R_{final}$ after placing all 50 buildings, it has no way to assign credit for that reward to the 50 individual actions. An agent that places the first 49 buildings perfectly and makes one mistake will receive a similar (poor) reward to an agent that makes 50 mistakes. This makes learning impossible.

5.2 Solution: Potential-Based Reward Shaping (PBRS)

The solution is to provide an immediate reward $r_t$ at every step. However, a naive reward (e.g., $r_t = \text{"distance to center"}$) will bias the agent, causing it to optimize for this proxy and fail at the true objective.42
The canonical solution is Potential-Based Reward Shaping (PBRS).42 This method guarantees that the optimal policy of the shaped-reward problem is identical to the optimal policy of the original sparse-reward problem.43
PBRS defines a new, shaped reward $R'$ as the sum of the original environment reward $R_{env}$ and a potential-based shaping term $F$:
$R'(s_t, a_t, s_{t+1}) = R_{env}(s_t, a_t) + F(s_t, s_{t+1})$
The shaping term $F$ is defined by a potential function $\Phi(s)$ that maps any state (a partial layout) to a scalar value:

$$F(s_t, s_{t+1}) = \gamma \Phi(s_{t+1}) - \Phi(s_t)$$
where $\gamma$ is the MDP's discount factor.

5.3 Designing the Potential Function $\Phi(s)$

This is the core of the solution. We define the potential function $\Phi(s_t)$ to be the scalarized multi-objective fitness of the current partial layout $s_t$.
Let $O_i(s_t)$ be the evaluation function for objective $i$ (e.g., total energy cost, adjacency score, constraint violations) given the partial layout $s_t$.
Let $w_i$ be the weight for that objective.
The potential function is:

$$\Phi(s_t) = f(s_t) = \sum_{i=1}^{M} w_i \cdot O_i(s_t)$$
With this definition, the immediate reward $r_t$ (ignoring the $R_{env}$ term, which is 0 until the end) becomes:

$$r_t = \gamma \Phi(s_{t+1}) - \Phi(s_t) = \gamma f(s_{t+1}) - f(s_t)$$
This $r_t$ represents the marginal improvement in total layout fitness caused by placing the last building. This signal is:
Dense: The agent receives a reward at every step.
Informative: It directly measures progress toward the true multi-objective goal.
Policy-Invariant: It does not introduce suboptimal bias.42

5.4 Multi-Objective Reward Aggregation

The function $f(s_t)$ must aggregate the 5-8 objectives into a single scalar.
Weighted Sum (WS): The $\sum w_i O_i$ approach is the simplest.45 It is easy to implement and understand. However, it is known to fail in finding solutions in non-convex regions of the Pareto front.47
Alternative Methods: For a more robust exploration of the Pareto front, advanced Multi-Objective RL (MORL) techniques can be used, such as finding a set of Pareto-optimal policies 48 or using non-linear scalarization functions like a weighted maximum.49 For most applications, training multiple agents with different weight vectors $w_i$ using WS is a practical starting point.

5.5 Handling Constraints

Hard Constraints: (e.g., out-of-bounds, building overlap) should immediately terminate the episode with a large negative reward (e.g., -1).
Soft Constraints: (e.g., "prefer classrooms to be <100m from library") should be formulated as a penalty term $O_i$ within the potential function $\Phi(s)$.

6. Advanced Exploration and Hierarchical Strategies

Even with PBRS, the high-dimensional state space can trap an agent in a local optimum. Advanced exploration strategies are necessary.

6.1 Entropy Regularization (via SAC)

As described in Section 3.3, SAC's maximum entropy objective is the first line of defense.27 By training the agent to maximize reward and entropy, it actively avoids policy collapse and is intrinsically motivated to explore diverse placement strategies.

6.2 Intrinsic Motivation (Curiosity-Driven RL)

In cases where the PBRS signal is still too sparse, an intrinsic reward can be added.51 The most common method is the Intrinsic Curiosity Module (ICM).53
Mechanism: The ICM adds a "curiosity" bonus $r_i$ to the extrinsic reward $r_t$.
Forward Model: An auxiliary network is trained to predict the next state $\hat{s}_{t+1}$ given $(s_t, a_t)$.
Curiosity Bonus: The bonus is proportional to the prediction error: $r_i \propto |
| \hat{s}{t+1} - s{t+1} ||^2$.
Effect: The agent is rewarded for taking actions that lead to surprising or novel states—parts of the layout it has not explored and does not yet understand. This drives the agent to explore unfamiliar parts of the state space.53

6.3 Hierarchical Reinforcement Learning (HRL)

HRL offers a powerful method to decompose the complex problem into a hierarchy of sub-tasks.54 This is a very natural fit for spatial planning.
Mechanism (Feudal RL / Options Framework):
High-Level Policy (Meta-Controller): $\pi_{hi}(g_t | s_t)$. This policy does not place buildings. It selects a subgoal $g_t$. For example, $g_t$ = "Place the classroom in the Academic Zone (top-right quadrant)".
Low-Level Policy (Controller): $\pi_{lo}(a_t | s_t, g_t)$. This policy is conditioned on the subgoal $g_t$ and learns to execute it (i.e., find the exact $(x, y)$ coordinates).57
Benefits: HRL simplifies the problem in two ways:
Action Space Decomposition: The meta-controller operates in a small, abstract action space (e.g., 4 zones $\times$ 7 building types = 28 actions). The low-level controller operates in a reduced continuous space (e.g., just $(x, y)$ coordinates within the selected zone).
Temporally-Extended Exploration: The meta-controller explores at the "zone" level, allowing the agent to rapidly test high-level strategies (e.g., "what if I put all academics in the north?") without needing to learn the low-level placements from scratch.56

7. Generalization: Transfer and Meta-Learning

Training a DRL agent is computationally expensive.59 A practical model must generalize to new sites and new problem instances (e.g., different building counts) without complete retraining. This directly addresses the validation challenge of training on 50-building problems and testing on 100-building problems.

7.1 Strategy 1: Transfer Learning (Pre-training and Fine-tuning)

Mechanism: This is a two-stage process.61
Pre-training: Train the DRL agent (e.g., the SAC model with its hybrid CNN-GNN architecture) on a large, diverse dataset of synthetic campus layouts.59
Fine-tuning: For a new, specific site, initialize the model with the pre-trained weights and fine-tune it on the new site's specific constraints.
What is Transferred: The encoder weights ($f_{cnn}, f_{gnn}$) are transferred.59 These networks have learned "general principles" of spatial planning (e.g., "how to identify free space," "how to evaluate adjacency") that are broadly applicable.

7.2 Strategy 2: Domain Randomization (DR)

This strategy is key to solving the (train 50, test 100) challenge. Instead of pre-training on static data, DR forces the policy to become robust by randomizing the environment parameters at the start of every training episode.
Parameters to Randomize:
Building Count: $N \sim \text{Uniform}(30, 100)$.
Site Size: $H, W \sim \text{Uniform}(80, 120)$.
Constraint Maps: Randomly generate "no-build" zones.
Objective Weights: Randomize the $w_i$ vector to expose the agent to different trade-offs.
By training on this fluctuating distribution of problems, the agent's policy $\pi(a|s)$ is forced to learn a generalizable strategy rather than overfitting to a single site configuration.

7.3 Strategy 3: Meta-Learning for Few-Shot Adaptation

Meta-learning ("learning to learn") trains an agent to adapt quickly to new tasks.65
Algorithms: MAML (Model-Agnostic Meta-Learning) 66 and Reptile.67 Reptile is simpler and more computationally efficient.67
Reptile Mechanism: Reptile learns a network initialization $\theta$ that is primed for rapid fine-tuning.67
Sample a task $T_i$ (e.g., a new site layout problem).
Train on $T_i$ for $k$ steps (e.g., 10 SGD steps), moving weights from $\theta \to W_i$.
Update the meta-weights: $\theta \leftarrow \theta + \epsilon (W_i - \theta)$.
Result: The final $\theta$ is not optimal for any single site, but is an "average" set of weights that can converge to a new, unseen site's optimal policy in just a few gradient steps, enabling true "few-shot adaptation".67

8. Comparative Analysis: DRL vs. Evolutionary and Hybrid Methods

DRL is a powerful paradigm, but not the only one. Evolutionary Algorithms (EAs) are also widely used for layout optimization, including hybrid approaches like H-SAGA (Hybrid Simulated Annealing Genetic Algorithm).70
DRL (e.g., SAC/PPO):
Pros: High sample efficiency (off-policy SAC).75 Learns a reusable policy, meaning inference is instantaneous (a single forward pass).76 Can leverage gradient information.
Cons: Prone to local optima; high sensitivity to hyperparameters 75; high initial training cost (often millions of episodes).60
EA (e.g., GA, H-SAGA):
Pros: Excellent global exploration capabilities due to population-based search.77 Robust to sparse rewards. Highly effective for complex NP-hard combinatorial problems.78
Cons: Extremely low sample efficiency (must evaluate every member of the population in every generation).75 Does not learn a "policy"; the optimization search must be re-run from scratch for every new problem instance.77
Hybrid RL + EA Approaches (The True SOTA):The strengths of DRL (gradient-based local search) and EAs (population-based global search) are complementary.77 The most advanced SOTA methods synergize them.83
Strategy 1: EA-guided RL: Use an EA to explore the global space, and its solutions are used to populate the replay buffer or guide the initial exploration of an RL agent.84
Strategy 2: RL-guided EA: This is the most promising direction.83
An EA (e.g., GA) maintains a population of full 50-building layouts.
The Mutation Operator is replaced by a DRL agent.86
The DRL agent is trained to take a full layout as state $s$ and output a "mutation" action $a$ (e.g., "swap building #23 and #45") that maximizes the fitness improvement. This is, in effect, a learned local search heuristic.

8.1 Table 1: Comparative Analysis of Optimization Approaches


Metric
DRL (SAC/PPO)
EA (GA/H-SAGA)
Hybrid (RL-guided EA)
Sample Efficiency
High (off-policy) / Low (on-policy) [31, 75]
Very Low 75
Medium
Global Exploration
Low to Medium
High 77
High
Solution Quality
Good (can get stuck in local optima)
Good (strong global search)
Excellent (SOTA)
Training Cost
High
High
Very High
Inference
Trivial (one-shot policy) 76
N/A (search is the inference)
N/A (search is the inference)
Interpretability
Low (black-box policy)
Medium (can analyze population)
Low
Primary Mechanism
Gradient-based Policy Optimization
Population-based Stochastic Search
Learned Heuristic Search

8.2 Training Convergence Analysis

A typical DRL training curve, plotting "Cumulative Reward vs. Training Steps," is characterized by high variance and instability.60
(Representative Figure) A plot would show the average reward (a solid line) and the standard deviation (a shaded region).
Phase 1 (Exploration): The reward is low and noisy as the agent explores randomly.
Phase 2 (Learning): The reward rapidly increases as the agent discovers the PBRS signal and basic strategies.
Phase 3 (Plateau): The reward plateaus, often with high variance, as the agent fine-tunes its policy and struggles to escape local optima.60
The goal of techniques like PBRS (Sec 5), entropy regularization (Sec 3.3), and stable algorithms like PPO/SAC is to make Phase 2 steeper (improving sample efficiency 89) and Phase 3 higher and more stable (improving final solution quality). EAs, by contrast, typically show a slower, steadier, and more monotonic increase in population-best fitness, but require orders of magnitude more environment evaluations.

9. State-of-the-Art Applications Review (2020-2025)

The application of DRL to spatial planning is an active research area.

9.1 Facility Layout Problems (FLP)

Heinbach et al. (2025) : "From theory to application: investigating the generalizability of facility layout problems using a deep reinforcement learning approach." This work directly addresses generalization. It uses PPO with a fixed-size, image-based state representation, allowing a single trained agent to generate efficient layouts for problems of different sizes (e.g., varying machine counts) without retraining.
Other Works 76: Recent papers have explored DRL for FLP, often focusing on real-time solutions for dynamic workshops 76 and integrating DRL with production simulations.91

9.2 Urban Morphology and Planning

Zheng et al. (2023) 92: "Spatial planning of urban communities via deep reinforcement learning." This paper directly tackles community-scale planning, formulating it as a DRL problem.
Al-Shaar et al. (2025) 93: "Urban morphology and climate vulnerability assessment in Kuwait: A spatio-temporal predictive analysis utilizing deep neural network-enhanced markov chain models..." (DOI: 10.1371/journal.pone.0318604). This study uses DNN-enhanced Markov models (a related, but not strictly RL, approach) to perform predictive modeling of urban growth patterns.
Other Works 94: Many recent generative approaches for urban form use Generative Adversarial Networks (GANs) and Deep Convolutional GANs (DCGANs) as an alternative to DRL, focusing on image-based generation of urban textures.

9.3 Floorplan Generation

Wang et al. (2024) 97: "Deep reinforcement learning for community architectural layout generation." This paper formulates the layout task as a Markov game, using Multi-Agent Reinforcement Learning (MARL) and curriculum learning to generate residential layouts. This is a significant SOTA deviation, where different "agents" (e.g., building types) cooperate or compete for space.
Shabani et al. (2023) 98: "House-diffusion: Vector floorplan generation via a diffusion model..." This highlights the primary competitor to DRL for generative design: diffusion models. These models are generative-first, producing layouts by "denoising" a random vector, and have shown remarkable results.
Anonymous (2024) 99: "A Deep Reinforcement Learning Floorplanning Algorithm Based on Sequence Pairs." This work defines the MDP differently. The state is a "sequence pair" (a common representation in chip design), and the actions are perturbations (e.g., "swap two modules," "rotate module").99 This is an example of DRL as a refinement policy rather than a generative one.

10. Implementation Guide and Code Templates

This section provides actionable templates for implementing the proposed DRL framework.

10.1 Recommended Python Ecosystem

Environment: gymnasium (the modern fork of OpenAI Gym).100
DRL Libraries:
stable-baselines3: Ideal for PPO and experimentation due to its simplicity.103
ray[rllib]: Ideal for SAC and large-scale, distributed training.105
ML Backend: PyTorch.

10.2 Code Template 1: Custom gymnasium.Env for Building Placement

This environment implements the hybrid state and PBRS reward logic.

Python


import gymnasium as gymimport numpy as npfrom gymnasium import spacesclass BuildingPlacementEnv(gym.Env):    """    Custom Gym Environment for Sequential Building Placement.    - State: Hybrid Dict {'grid', 'graph', 'context'}    - Action: Continuous Box (x, y, type_logits)    - Reward: Potential-Based Reward Shaping (PBRS)    """    metadata = {"render_modes": ["human", "rgb_array"]}    def __init__(self, grid_size=(100, 100), num_building_types=7, buildings_to_place=50):        super().__init__()                self.grid_size = grid_size        self.num_types = num_building_types        self.total_buildings = buildings_to_place        self.buildings_queue = # List of building features (e.g., type, size)                self.H, self.W = grid_size        self.C = 4 # Example channels: (occupancy, type_id, constraints, proximity)        # 1. OBSERVATION SPACE (Hybrid)        self.observation_space = spaces.Dict(            {                # 2D Grid (CNN input)                "grid": spaces.Box(low=0, high=1, shape=(self.C, self.H, self.W), dtype=np.float32),                                # Relational Graph (GNN input) - Simplified as a feature matrix                # A full GNN env would use torch_geometric.data.Data                # Here, we use a fixed-size adjacency matrix and node feature list                "nodes": spaces.Box(low=0, high=1, shape=(self.total_buildings, 5), dtype=np.float32), # 5 features per node                "adj": spaces.Box(low=0, high=1, shape=(self.total_buildings, self.total_buildings), dtype=np.float32),                                # Context (MLP input) - Features of the *next* building                "context": spaces.Box(low=0, high=1, shape=(self.num_types + 2,), dtype=np.float32) # e.g., one-hot type + size            }        )                # 2. ACTION SPACE (Continuous)        # (x_coord, y_coord, type_logit_1,..., type_logit_7)        # We assume the building type is given by the context k_t, so action is just (x, y)        action_dim = 2 # (x, y)        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(action_dim,), dtype=np.float32)        self.current_step = 0        self.current_layout = None # Represents the set of placed buildings        self.current_potential = 0.0    def _get_obs(self):        # --- Logic to build the hybrid state ---        # 1. Create the grid tensor G_t        grid_obs = np.zeros((self.C, self.H, self.W), dtype=np.float32)        #... (fill grid_obs based on self.current_layout)...                # 2. Create the graph tensors V_t        node_obs = np.zeros((self.total_buildings, 5), dtype=np.float32)        adj_obs = np.zeros((self.total_buildings, self.total_buildings), dtype=np.float32)        #... (fill node_obs and adj_obs based on self.current_layout)...                # 3. Get the context vector k_t        context_obs = self.buildings_queue[self.current_step]                return {"grid": grid_obs, "nodes": node_obs, "adj": adj_obs, "context": context_obs}    def _calculate_potential(self, layout):        """Calculates the multi-objective fitness (Phi) of a layout."""        # This is the core reward logic        # Phi(s_t) = w1*O1(s_t) + w2*O2(s_t) +...                # Objective 1: Minimize constraint violations        o1_penalty = 0.0        #... (check layout for violations)...                # Objective 2: Maximize adjacency score        o2_score = 0.0        #... (check required adjacencies)...                # Weighted sum scalarization [46]        w1, w2 = -10.0, 5.0 # Example weights        return (w1 * o1_penalty) + (w2 * o2_score)    def reset(self, seed=None, options=None):        super().reset(seed=seed)                # 1. Reset building queue (e.g., 50 buildings)        # (In a real scenario, this would be loaded from a problem instance)        self.buildings_queue = [np.random.rand(self.num_types + 2) for _ in range(self.total_buildings)]                # 2. Reset state        self.current_step = 0        self.current_layout = # List of (building, x, y) tuples                # 3. Calculate initial potential Phi(s_0)        self.current_potential = self._calculate_potential(self.current_layout)                obs = self._get_obs()        info = {} # Info dict                return obs, info    def step(self, action):        # 1. Parse the action (scale from [-1, 1] to grid coordinates)        x_coord = int((action + 1) / 2 * (self.W - 1))        y_coord = int((action + 1) / 2 * (self.H - 1))                building_to_place = self.buildings_queue[self.current_step]                # 2. Check for hard constraint violations (e.g., overlap, out-of-bounds)        terminated = False        if self._check_violation(x_coord, y_coord, building_to_place):            terminated = True            reward = -1.0 # Large penalty                    else:            # 3. Apply action: Place the building            self.current_layout.append((building_to_place, x_coord, y_coord))            self.current_step += 1                        # 4. Calculate PBRS Reward: r_t = gamma * Phi(s_{t+1}) - Phi(s_t)             gamma = 0.99 # Must match the agent's gamma            new_potential = self._calculate_potential(self.current_layout)            reward = (gamma * new_potential) - self.current_potential                        # Update the potential for the next step            self.current_potential = new_potential                        # 5. Check for termination (all buildings placed)            if self.current_step == self.total_buildings:                terminated = True                # (Optional) Add final bonus                reward += 10.0 # Bonus for successful completion                obs = self._get_obs()        truncated = False # Not time-limited        info = {}                return obs, reward, terminated, truncated, info    def _check_violation(self, x, y, building):        #... (Logic to check for OOB or overlaps in self.current_layout)...        return False # Placeholder

10.3 Code Template 2: Training Script (PPO + stable-baselines3)

Note: This template assumes the BuildingPlacementEnv action space is modified to be spaces.Discrete(70000), as PPO does not natively support hybrid observation spaces with a custom policy without more complex setup.

Python


import gymnasium as gymfrom stable_baselines3 import PPOfrom stable_baselines3.common.env_util import make_vec_envfrom stable_baselines3.common.policies import MultiInputPolicyfrom building_env import BuildingPlacementEnv # Assumes env is discrete# Register the environmentgym.register(    id='BuildingPlacement-v0',    entry_point='building_env:BuildingPlacementEnv',    kwargs={'grid_size': (100, 100), 'num_building_types': 7, 'buildings_to_place': 50})# Parallel environments [104]num_cpu = 8vec_env = make_vec_env('BuildingPlacement-v0', n_envs=num_cpu)# NOTE: A standard "MlpPolicy" or "CnnPolicy" will FAIL.# The hybrid Dict observation space requires a "MultiInputPolicy" [103]# and a custom feature extractor.# For simplicity, we use the default policy, but a custom one is needed.# Recommended PPO Hyperparameters model = PPO(    MultiInputPolicy,  # Use the policy designed for Dict spaces    vec_env,    verbose=1,    n_steps=2048,        # Larger rollout buffer for stable gradients    batch_size=64,       # Smaller minibatch size    n_epochs=10,    gamma=0.99,    learning_rate=3e-4,    tensorboard_log="./ppo_placement_tensorboard/")print("Starting PPO training...")model.learn(total_timesteps=10_000_000)model.save("ppo_building_placement")print("Training complete.")

10.4 Code Template 3: Training Script (SAC + ray[rllib])

This template uses the continuous action space as defined in BuildingPlacementEnv (Template 1), which is the recommended SOTA approach.

Python


import rayfrom ray.rllib.algorithms.sac import SACConfigfrom building_env import BuildingPlacementEnv# Initialize Rayray.init()# 1. Configure the SAC Algorithm [106, 109]config = (    SACConfig()   .environment(        env=BuildingPlacementEnv,         env_config={            "grid_size": (100, 100),            "num_building_types": 7,            "buildings_to_place": 50        }    )   .env_runners(num_env_runners=4) # Parallel workers   .training(        # SAC Hyperparameters        gamma=0.99,        actor_lr=3e-4,        critic_lr=3e-4,        train_batch_size=256,        replay_buffer_config={            "type": "MultiAgentReplayBuffer",            "capacity": 1_000_000, # Large replay buffer        },        # Model config for the hybrid CNN-GNN-MLP        model={            "custom_model": "my_hybrid_model", # Requires registering a custom model            # (RLlib default model will fail on Dict space without custom model)            "fcnet_hiddens": ,        },        # Entropy regularization (the 'alpha' in SAC)        "initial_alpha": 1.0,        "optimization": {            "actor_learning_rate": 3e-4,            "critic_learning_rate": 3e-4,            "entropy_learning_rate": 3e-4,        },    )   .framework("torch"))# 2. Build and Train the Algorithm [106]algo = config.build()print("Starting SAC training...")for i in range(1000):    result = algo.train()    print(f"Iteration: {i}, Mean Reward: {result['env_runners']['episode_reward_mean']}")    if i % 100 == 0:        checkpoint_dir = algo.save()        print(f"Checkpoint saved in {checkpoint_dir}")print("Training complete.")ray.shutdown()

10.5 Hyperparameter Recommendations

Normalize Observations/Actions: Always normalize your observation space.110 For stable-baselines3, use the VecNormalize wrapper. The continuous action space should be rescaled to [-1, 1].
Discount Factor (gamma): Must be high (e.g., 0.99 or 0.999) for PBRS to work correctly, as $r_t = \gamma \Phi(s_{t+1}) - \Phi(s_t)$.
PPO: n_steps (rollout buffer size) is critical. A larger value (e.g., 2048 or 4096) provides more stable gradients. batch_size should be small (e.g., 64, 128).108
SAC: buffer_size (large, e.g., 1e6), batch_size (e.g., 256, 512), learning_rate (low, e.g., 3e-4).
Network: Start with smaller networks (e.g., two 256-unit hidden layers) and scale up. Use the Adam optimizer with a learning rate schedule (e.g., linear decay).

11. Conclusion and Future Research Directions


11.1 Summary of Findings

This report has detailed a comprehensive DRL framework for sequential spatial planning. The problem, characterized by a massive combinatorial action space and sparse, multi-objective rewards, can be made tractable through a series of key technical decisions:
Action Space Reformulation: The naive 70,000-action discrete space is intractable. It should be reformulated as a low-dimensional continuous action space, which embeds the problem's spatial structure.
Algorithm Selection: This continuous formulation enables the use of Soft Actor-Critic (SAC), a highly sample-efficient off-policy algorithm. Its built-in entropy regularization is critical for driving the exploration needed for generative design.
State Representation: The state must be a hybrid, capturing both spatial and relational information. The SOTA network architecture is a hybrid CNN-GNN model that fuses spatial grid features (from a CNN) with relational graph features (from a GNN).
Reward Engineering: The sparse reward problem is solved using Potential-Based Reward Shaping (PBRS). By defining the potential $\Phi(s)$ as the scalarized fitness of the partial layout, the agent receives a dense, non-biasing reward signal at every step, corresponding to the marginal improvement in layout quality.

11.2 Future Research Directions

While the proposed SAC-based framework is SOTA, research in this area is rapidly evolving.
Diffusion Models: As an alternative to sequential RL, generative-first models like diffusion (as seen in House-diffusion 98) show immense promise. These models learn to generate an entire layout at once by "denoising" a random input.
Multi-Agent Reinforcement Learning (MARL): Formulating the problem as a "Markov Game" 97, where each building type (or each individual building) is an independent agent competing for space, is a powerful and flexible paradigm.
Hybrid RL-EA Methods: The true SOTA for robust, global optimization likely lies in the synergy of DRL and EAs. Future work should focus on RL-guided genetic operators 83, where a DRL agent learns an intelligent, adaptive "mutation policy" to improve a population of layouts managed by a Genetic Algorithm.
Alıntılanan çalışmalar
(PDF) From theory to application: investigating the generalizability of ..., erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/392162513_From_theory_to_application_investigating_the_generalizability_of_facility_layout_problems_using_a_deep_reinforcement_learning_approach
Illuminating Spaces Deep Reinforcement Learning and Laser-Wall Partitioning for Architectural Layout Generation - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2502.04407v1
Markov decision process - Wikipedia, erişim tarihi Kasım 3, 2025, https://en.wikipedia.org/wiki/Markov_decision_process
Markov Decision Process: A Practical Road‑map for Sequential Decision‑Making - Strategic Engineering, erişim tarihi Kasım 3, 2025, https://strategic-engineering.co/blog/concepts/markov-decision-processes/
Markov Decision Processes, erişim tarihi Kasım 3, 2025, https://thedecisionlab.com/reference-guide/statistics/markov-decision-processes
Understanding the Markov Decision Process (MDP) - Built In, erişim tarihi Kasım 3, 2025, https://builtin.com/machine-learning/markov-decision-process
Are policy gradient methods good for large discrete action spaces? - AI Stack Exchange, erişim tarihi Kasım 3, 2025, https://ai.stackexchange.com/questions/27849/are-policy-gradient-methods-good-for-large-discrete-action-spaces
Does DQN fit well with large discrete action space? or Generalize well? - Reddit, erişim tarihi Kasım 3, 2025, https://www.reddit.com/r/reinforcementlearning/comments/rfbiti/does_dqn_fit_well_with_large_discrete_action/
Comprehensive Overview of Reward Engineering and Shaping in Advancing Reinforcement Learning Applications - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2408.10215v1
Reward Shaping for Faster Learning in Reinforcement Learning - CodeSignal, erişim tarihi Kasım 3, 2025, https://codesignal.com/learn/courses/advanced-rl-techniques-optimization-and-beyond/lessons/reward-shaping-for-faster-learning-in-reinforcement-learning
Policy Optimization with Smooth Guidance from Sparse-Reward Demonstrations - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2401.00162v1
Integrating Occupancy Grids with Spatial-Temporal Reinforcement Learning for Enhanced Control - ROSA P, erişim tarihi Kasım 3, 2025, https://rosap.ntl.bts.gov/view/dot/86174/dot_86174_DS1.pdf
Multimodal Deep Reinforcement Learning with Auxiliary Task for Obstacle Avoidance of Indoor Mobile Robot - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/1424-8220/21/4/1363
Transformation & Translation Occupancy Grid Mapping: 2-Dimensional Deep Learning Refined SLAM - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2504.19654v1
4.1: Map Representations - Engineering LibreTexts, erişim tarihi Kasım 3, 2025, https://eng.libretexts.org/Bookshelves/Mechanical_Engineering/Introduction_to_Autonomous_Robots_(Correll)/04%3A_Path_Planning/4.01%3A_Map_Representations
Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2508.10747v2
Exploring Graph Neural Networks in Reinforcement Learning: A Comparative Study on Architectures for Locomotion Tasks - ScholarWorks @ UTRGV, erişim tarihi Kasım 3, 2025, https://scholarworks.utrgv.edu/cgi/viewcontent.cgi?article=2498&context=etd
A Comparative Study of Deep Reinforcement Learning Algorithms for Urban Autonomous Driving: Addressing the Geographic and Regulatory Challenges in CARLA - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2076-3417/15/12/6838
[1810.06394] Parametrized Deep Q-Networks Learning: Reinforcement Learning with Discrete-Continuous Hybrid Action Space - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/abs/1810.06394
Hybrid Deep Reinforcement Learning Considering Discrete-Continuous Action Spaces for Real-Time Energy Management in More Electric Aircraft - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/1996-1073/15/17/6323
Deep Multi-Agent Reinforcement Learning with Discrete-Continuous Hybrid Action Spaces, erişim tarihi Kasım 3, 2025, https://www.ijcai.org/proceedings/2019/323
Off-Policy Reinforcement Learning with Delayed Rewards, erişim tarihi Kasım 3, 2025, https://bimsa.net/doc/publication/4533.pdf
Efficient Multi-Objective Optimization on Dynamic Flexible Job Shop Scheduling Using Deep Reinforcement Learning Approach - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2227-9717/11/7/2018
Comparative Study of Reinforcement Learning Algorithms: Deep Q-Networks, Deep Deterministic Policy Gradients and Proximal Policy Optimization - Washington University Open Scholarship, erişim tarihi Kasım 3, 2025, https://openscholarship.wustl.edu/cgi/viewcontent.cgi?article=1017&context=eseundergraduate_research
PPO — Stable Baselines3 2.7.1a3 documentation, erişim tarihi Kasım 3, 2025, https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html
Proximal policy optimization - Wikipedia, erişim tarihi Kasım 3, 2025, https://en.wikipedia.org/wiki/Proximal_policy_optimization
Soft Actor-Critic — Spinning Up 文档 - 深度强化学习教程, erişim tarihi Kasım 3, 2025, https://spinningup.readthedocs.io/zh_CN/latest/algorithms/sac.html
A Comparison of PPO, TD3 and SAC Reinforcement Algorithms for Quadruped Walking Gait Generation - Scirp.org., erişim tarihi Kasım 3, 2025, https://www.scirp.org/journal/paperinformation?paperid=123401
Performance Comparison of Deep RL Algorithms for Energy Systems Optimal Scheduling - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/pdf/2208.00728
Reinforcement Learning Tutorial: Soft Actor-Critic (SAC) | by Jiawei Kong | Medium, erişim tarihi Kasım 3, 2025, https://medium.com/@jiawei.kong/reinforcement-learning-tutorial-soft-actor-critic-sac-f0a097b328c3
Understanding SAC: Data Collection and Training - RLlib - Ray, erişim tarihi Kasım 3, 2025, https://discuss.ray.io/t/understanding-sac-data-collection-and-training/11931
Architecture of the CNN–MLP reward estimator used in the proposed DRL... - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/figure/Architecture-of-the-CNN-MLP-reward-estimator-used-in-the-proposed-DRL-framework-The-CNN_fig3_394781329
The architecture of our CNN-based dueling DQN network. This network... | Download Scientific Diagram - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/figure/The-architecture-of-our-CNN-based-dueling-DQN-network-This-network-takes-three-local_fig7_353996671
Mobile Robots Exploration via Deep Reinforcement Learning - OpenReview, erişim tarihi Kasım 3, 2025, https://openreview.net/pdf?id=WEkiK99gyvj
Evaluating Hybrid Cnn-Mlp Architecture For Analyzing Novel Network Traffic Attacks - International Journal of Scientific & Technology Research, erişim tarihi Kasım 3, 2025, https://www.ijstr.org/final-print/mar2020/Evaluating-Hybrid-Cnn-mlp-Architecture-For-Analyzing-Novel-Network-Traffic-Attacks.pdf
Multi-Agent Deep Reinforcement Learning for Large-Scale Traffic Signal Control with Spatio-Temporal Attention Mechanism - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2076-3417/15/15/8605
Graph Neural Networks and Deep Reinforcement Learning Based Resource Allocation for V2X Communications - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2407.06518v1
Optimized Unet with Attention Mechanism for Multi-Scale Semantic Segmentation - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/abs/2502.03813
Spatial layout optimization model integrating layered attention mechanism in the development of smart tourism management - PMC - NIH, erişim tarihi Kasım 3, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC11623231/
Pay Attention to What and Where? Interpretable Feature Extractor in Vision-based Deep Reinforcement Learning - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2504.10071v1
Integrating attention mechanism and multi-scale feature extraction for fall detection - PMC, erişim tarihi Kasım 3, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC11145491/
1Some results are known for approximate ... - People @EECS, erişim tarihi Kasım 3, 2025, https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf
Improving the Effectiveness of Potential-Based Reward Shaping in Reinforcement Learning, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2502.01307v1
Potential Based Reward Shaping for Hierarchical Reinforcement Learning - IJCAI, erişim tarihi Kasım 3, 2025, https://www.ijcai.org/Proceedings/15/Papers/493.pdf
(PDF) An Empirical Comparison of Immediate and Delayed Rewards in Multi-Objective Q-Learning - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/384282328_An_Empirical_Comparison_of_Immediate_and_Delayed_Rewards_in_Multi-Objective_Q-Learning
Scalarizing Multi-Objective Robot Planning Problems using Weighted Maximization - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2312.07227v1
Why is the reward in reinforcement learning always a scalar? - AI Stack Exchange, erişim tarihi Kasım 3, 2025, https://ai.stackexchange.com/questions/22900/why-is-the-reward-in-reinforcement-learning-always-a-scalar
Multi-Objective Optimization of Energy Saving and Throughput in Heterogeneous Networks Using Deep Reinforcement Learning - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/1424-8220/21/23/7925
Scalarizing Multi-Objective Robot Planning Problems using Weighted Maximization - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/abs/2312.07227
Multi-Objective Reinforcement Learning with Non-Linear Scalarization - IFAAMAS, erişim tarihi Kasım 3, 2025, https://www.ifaamas.org/Proceedings/aamas2022/pdfs/p9.pdf
Curiosity-Driven Exploration (Chapter 3) - The Drive for Knowledge, erişim tarihi Kasım 3, 2025, https://www.cambridge.org/core/books/drive-for-knowledge/curiositydriven-exploration/D545C1583CE3E0765BB9E5F6C5766268
Curiosity-Driven Reinforcement Learning from Human Feedback - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2501.11463v1
Curiosity-Driven Exploration in Reinforcement Learning | by Amit Yadav | Biased-Algorithms, erişim tarihi Kasım 3, 2025, https://medium.com/biased-algorithms/curiosity-driven-exploration-in-reinforcement-learning-dd3f7d263fce
Hierarchical Reinforcement Learning: A Survey and Open Research Challenges - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2504-4990/4/1/9
Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation with Autonomous Mobile Robots - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2504.18794v1
Model-based hierarchical reinforcement learning and human action control - PMC, erişim tarihi Kasım 3, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC4186233/
Hierarchical Reinforcement Learning Based on Planning Operators* - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2309.14237v2
Hierarchical Reinforcement Learning (HRL): Breaking Down Complex Tasks | by Hey Amit, erişim tarihi Kasım 3, 2025, https://medium.com/@heyamit10/hierarchical-reinforcement-learning-hrl-breaking-down-complex-tasks-d9798e49c782
Online transfer learning strategy for enhancing the scalability and ..., erişim tarihi Kasım 3, 2025, https://buildings.lbl.gov/publications/online-transfer-learning-strategy
Sample Efficient Deep Reinforcement Learning for Control - TU Delft Research Portal, erişim tarihi Kasım 3, 2025, https://research.tudelft.nl/files/67523170/phd_thesis_timdebruin.pdf
Full article: Online transfer learning (OTL) for accelerating deep reinforcement learning (DRL) for building energy management - Taylor & Francis Online, erişim tarihi Kasım 3, 2025, https://www.tandfonline.com/doi/full/10.1080/19401493.2025.2511826
Online transfer learning (OTL) for accelerating deep reinforcement learning (DRL) for building energy management - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/392331110_Online_transfer_learning_OTL_for_accelerating_deep_reinforcement_learning_DRL_for_building_energy_management
One for Many: Transfer Learning for Building HVAC Control - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/pdf/2008.03625
(PDF) One for Many: Transfer Learning for Building HVAC Control - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/347579690_One_for_Many_Transfer_Learning_for_Building_HVAC_Control
Distributed Reptile Algorithm for Meta-Learning Over Multi-Agent Systems - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/365461342_Distributed_Reptile_Algorithm_for_Meta-Learning_Over_Multi-Agent_Systems
A Search for Efficient Meta-Learning: MAMLs, Reptiles, and Related Species - Medium, erişim tarihi Kasım 3, 2025, https://medium.com/data-science/a-search-for-efficient-meta-learning-mamls-reptiles-and-related-species-e47b8fc454f2
Reptile: A scalable meta-learning algorithm | OpenAI, erişim tarihi Kasım 3, 2025, https://openai.com/index/reptile/
[N] OpenAI Releases "Reptile", A Scalable Meta-Learning Algorithm - Includes an Interactive Tool to Test it On-site : r/MachineLearning - Reddit, erişim tarihi Kasım 3, 2025, https://www.reddit.com/r/MachineLearning/comments/82pwlw/n_openai_releases_reptile_a_scalable_metalearning/
UAV Control Method Combining Reptile Meta-Reinforcement Learning and Generative Adversarial Imitation Learning - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/1999-5903/16/3/105
Spatial layout and optimization of e-commerce logistics management based on combinatorial optimization algorithm - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/380767296_Spatial_layout_and_optimization_of_e-commerce_logistics_management_based_on_combinatorial_optimization_algorithm
[PDF] Hybrid Architecture of Genetic Algorithm and Simulated Annealing | Semantic Scholar, erişim tarihi Kasım 3, 2025, https://www.semanticscholar.org/paper/Hybrid-Architecture-of-Genetic-Algorithm-and-Yoshikawa-Yamauchi/ad3af7072d947eda79ca7ad93a0078e48d2e1812
Hybrid Simulated Annealing and Genetic Algorithms for Industrial Production Management Problems | Request PDF - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/253376933_Hybrid_Simulated_Annealing_and_Genetic_Algorithms_for_Industrial_Production_Management_Problems
Hybrid Genetic Simulated Annealing Algorithm for Improved Flow Shop Scheduling with Makespan Criterion - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2076-3417/8/12/2621
Flowchart of the H-SAGA algorithm. | Download Scientific Diagram, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/figure/Flowchart-of-the-H-SAGA-algorithm_fig6_394405729
A Dimensional Comparison between Evolutionary Algorithm and ..., erişim tarihi Kasım 3, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC8074202/
Workshop Facility Layout Optimization Based on Deep Reinforcement Learning - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2227-9717/12/1/201
arxiv.org, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2401.11963v4
SAGA: Synthesis Augmentation with Genetic Algorithms for In-Memory Sequence Optimization Identify applicable funding agency here. If none, delete this. - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2406.09677v1
Optimizing Layout Using Spatial Quality Metrics and User Preferences - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/319441052_Optimizing_Layout_Using_Spatial_Quality_Metrics_and_User_Preferences
New improved hybrid genetic algorithm for optimizing facility layout design of reconfigurable manufacturing system - PubMed, erişim tarihi Kasım 3, 2025, https://pubmed.ncbi.nlm.nih.gov/40596689/
Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and Research Opportunities - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2308.13420v3
(PDF) A Sample-Efficiency Comparison Between Evolutionary Algorithms and Deep Reinforcement Learning for Path Planning in an Environmental Patrolling Mission - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/350726014_A_Sample-Efficiency_Comparison_Between_Evolutionary_Algorithms_and_Deep_Reinforcement_Learning_for_Path_Planning_in_an_Environmental_Patrolling_Mission
Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2506.09404v1
Comparative Benchmark of Sampling-Based and DRL Motion Planning Methods for Industrial Robotic Arms - PMC - PubMed Central, erişim tarihi Kasım 3, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC12431259/
A Guide to Genetic 'Learning' Algorithms for Optimization | Towards Data Science, erişim tarihi Kasım 3, 2025, https://towardsdatascience.com/a-guide-to-genetic-learning-algorithms-for-optimization-e1067cdc77e7/
Using Reinforcement Learning for Tuning Genetic Algorithms - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/351874169_Using_Reinforcement_Learning_for_Tuning_Genetic_Algorithms
Reinforcement Learning for Mutation Operator Selection in Automated Program Repair, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2306.05792v2
Sample-Efficient Deep Reinforcement Learning for Continuous Control - ProQuest, erişim tarihi Kasım 3, 2025, https://search.proquest.com/openview/260ad408e79f0c893f6100b916f03073/1?pq-origsite=gscholar&cbl=18750&diss=y
Improving Sample Efficiency in Deep Reinforcement Learning Based Control of Dynamic Systems School of Engineering - Open Research Newcastle, erişim tarihi Kasım 3, 2025, https://openresearch.newcastle.edu.au/ndownloader/files/54406676
arXiv:2503.01069v1 [cs.AI] 3 Mar 2025, erişim tarihi Kasım 3, 2025, https://arxiv.org/pdf/2503.01069
Optimization of the Factory Layout and Production Flow Using Production-Simulation-Based Reinforcement Learning - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/381211712_Optimization_of_the_Factory_Layout_and_Production_Flow_Using_Production-Simulation-Based_Reinforcement_Learning
Artificial Intelligence Training as a Research Method in Urban Design, Especially for Hyper-Morphology - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/396778400_Artificial_Intelligence_Training_as_a_Research_Method_in_Urban_Design_Especially_for_Hyper-Morphology
Urban morphology and climate vulnerability assessment in Kuwait: A spatio-temporal predictive analysis utilizing deep neural network-enhanced markov chain models for 2050 and 2100 | PLOS One - Research journals, erişim tarihi Kasım 3, 2025, https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0318604
A Generative Urban Form Design Framework Based on Deep Convolutional GANs and Landscape Pattern Metrics for Sustainable Renewal in Highly Urbanized Cities - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2071-1050/17/10/4548
(PDF) A Generative Urban Form Design Framework Based on Deep Convolutional GANs and Landscape Pattern Metrics for Sustainable Renewal in Highly Urbanized Cities - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/391829356_A_Generative_Urban_Form_Design_Framework_Based_on_Deep_Convolutional_GANs_and_Landscape_Pattern_Metrics_for_Sustainable_Renewal_in_Highly_Urbanized_Cities
Urban Morphology - ISUF 2025, erişim tarihi Kasım 3, 2025, https://www.isuf2025.org/wp-content/uploads/2025/06/DRAFT_BoA_ISUF2025_250618.pdf
Deep reinforcement learning for community architectural layout ..., erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/386549567_Deep_reinforcement_learning_for_community_architectural_layout_generation
FloorPlan-DeepSeek (FPDS) - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/pdf/2506.21562
A Deep Reinforcement Learning Floorplanning Algorithm Based on ..., erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2076-3417/14/7/2905
How to create a custom Reinforcement Learning Environment in Gymnasium with Ray, erişim tarihi Kasım 3, 2025, https://ruslanmv.com/blog/How-to-create-a-custom-Reinforcement-Learning-Environment-in-Gym-and-Ray
OpenAI Gym: Creating Custom Gym Environments | DigitalOcean, erişim tarihi Kasım 3, 2025, https://www.digitalocean.com/community/tutorials/creating-custom-environments-openai-gym
Understanding action & observation spaces in gym for custom environments and agents, erişim tarihi Kasım 3, 2025, https://stackoverflow.com/questions/76289764/understanding-action-observation-spaces-in-gym-for-custom-environments-and-age
Examples — Stable Baselines3 2.7.1a3 documentation - Read the Docs, erişim tarihi Kasım 3, 2025, https://stable-baselines3.readthedocs.io/en/master/guide/examples.html
PPO — Stable Baselines3 2.1.0 documentation - Read the Docs, erişim tarihi Kasım 3, 2025, https://stable-baselines3.readthedocs.io/en/v2.1.0/modules/ppo.html
How to Use Action Mask with SAC in Ray RLlib | by Kaige - Medium, erişim tarihi Kasım 3, 2025, https://medium.com/@kaige.yang0110/ray-rllib-how-to-use-action-masking-and-complex-action-space-observation-sapce-with-built-in-644aecbeb35d
RLlib: Industry-Grade, Scalable Reinforcement Learning — Ray 2.51.0, erişim tarihi Kasım 3, 2025, https://docs.ray.io/en/latest/rllib/index.html
How to set configuration in tune for sac algorithm? - RLlib - Ray, erişim tarihi Kasım 3, 2025, https://discuss.ray.io/t/how-to-set-configuration-in-tune-for-sac-algorithm/412
Tips and Tricks for RL from Experimental Data using Stable Baselines3 Zoo - Reddit, erişim tarihi Kasım 3, 2025, https://www.reddit.com/r/reinforcementlearning/comments/vqb2wu/tips_and_tricks_for_rl_from_experimental_data/
Reinforcement Learning Tips and Tricks — Stable Baselines3 2.7 ..., erişim tarihi Kasım 3, 2025, https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html
