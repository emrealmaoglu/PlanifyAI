
Technical Implementation Guide: NSGA-III and MOEA/D for Many-Objective Spatial Planning

Executive Summary
This report provides a comprehensive technical guide for the implementation of many-objective evolutionary algorithms (MaOEAs), specifically NSGA-III and MOEA/D, for a 5-objective spatial planning problem. It covers the complete pipeline, from problem formulation and objective quantification to deep-level algorithmic mechanics, practical Python implementation using the pymoo library, and a robust framework for performance benchmarking and analysis. This guide provides detailed mathematical formulations, pseudocode, and code snippets, synthesizing state-of-the-art research. The report culminates in a direct comparative analysis, offering expert recommendations for algorithm selection and parameterization tailored to the unique challenges of spatial optimization, such as non-linear trade-offs, heterogeneous objectives, and complex, potentially disconnected Pareto fronts.

I. Formulating the 5-Objective Campus Spatial Planning Problem

This section establishes the mathematical foundation for the optimization problem. The core challenge in spatial planning is translating abstract goals such as "walkability" and "compactness" into quantifiable, computable objective functions and a valid genetic representation.

A. Decision Variables and Spatial Representation (The Genotype)

The genome of an evolutionary algorithm for this problem must encode a physical campus layout. This representation is non-trivial and dictates the effectiveness of the genetic operators.
	•	Grid-Based: The campus area is divided into a 2D grid ($N \times M$ cells). The genotype is a vector of length $N \times M$, where each gene $i$ represents a cell and its integer value encodes the land-use type (e.g., 0=Green Space, 1=Residential, 2=Academic).
	•	Graph-Based: The genotype represents an adjacency matrix or edge list, where nodes are functional units (buildings) and edges represent pathways.1 This is powerful for optimizing network-dependent objectives.
	•	Parametric: The genotype is a vector of parameters (e.g., $x, y$ coordinates, building height) that feed into a generative design script.2
For this guide, a Grid-Based representation is assumed due to its simplicity and direct mapping to compatibility and compactness objectives. Network-dependent objectives (walkability, cost) will be evaluated by running pathfinding algorithms (e.g., A*) on the grid in the objective function.

B. Objective Function Quantification (The Phenotype)

The 5-objective campus planning problem, based on literature 3, is formulated. All objectives are defined as minimization problems.
	•	$f_1$: Minimize Infrastructure Cost (Maximize Economy)
	•	Basis: Costs are driven by the length of new infrastructure (roads, utilities) and the economic value of land allocation.6
	•	Formulation 3:  $$f_1(\mathbf{x}) = \sum_{r \in R} C_{\text{road}} \cdot \text{length}(r) + \sum_{b \in B} C_{\text{utility}} \cdot \text{dist}(b, \text{main\_grid}) + \sum_{i,j} C_{\text{land\_value}}(i,j) \cdot \delta(i,j)$$  Where $\mathbf{x}$ is the layout, $R$ is the required road network, $B$ is the set of buildings, and $C_{\text{land\_value}}$ is the economic cost/value of allocating land use $\delta$ at cell $(i,j)$.3
	•	$f_2$: Minimize Incompatibility (Maximize Type Compatibility)
	•	Basis: Adjacency of certain land uses is penalized (e.g., residential vs. commercial), while others are rewarded (e.g., residential vs. park). This is modeled using a compatibility matrix.9
	•	Formulation 3:  $$f_2(\mathbf{x}) = \sum_{i,j} \sum_{i',j' \in \text{Neighbors}(i,j)} \text{Penalty}(T(i,j), T(i',j'))$$  Where $T(i,j)$ is the land-use type of cell $(i,j)$ and $\text{Penalty}(\cdot, \cdot)$ is a lookup matrix defining the cost of two types being adjacent.
	•	$f_3$: Minimize Lack of Compactness (Maximize Compactness)
	•	Basis: Compact development is more efficient.10 This objective maximizes shared borders of similar land-use types.3
	•	Formulation (adapted from 3): We minimize the inverse of the shared-border ratio.  $$f_3(\mathbf{x}) = \sum_{k \in K} \left( 1 - \frac{\sum_{i,j \in \text{Type}_k} \text{SharedBorders}(i,j)}{4 \cdot |\text{Type}_k|} \right)$$  Where $K$ is the set of land-use types, $\text{Type}_k$ is the set of cells of type $k$, and $\text{SharedBorders}(i,j)$ counts neighbors of the same type $k$.
	•	$f_4$: Minimize Lack of Walkability (Maximize Walkability)
	•	Basis: Walkability is defined by the network-based accessibility of destinations.13
	•	Formulation (adapted from 3):  $$f_4(\mathbf{x}) = \frac{1}{|R_c|} \sum_{i \in R_c} \sum_{j \in A_c} w_j \cdot \text{dist}_{\text{network}}(i, j)$$  Where $R_c$ is the set of residential cells, $A_c$ is the set of amenity cells, $w_j$ is a weight, and $\text{dist}_{\text{network}}(i, j)$ is the pathfinding distance (e.g., A* or Dijkstra), not the Euclidean distance.13
	•	$f_5$: Minimize Lack of Green Access (Maximize Green Access)
	•	Basis: A specific goal of planning is ensuring access to green spaces, which often conflicts with compactness.10
	•	Formulation (adapted from 3):  $$f_5(\mathbf{x}) = \frac{1}{|R_c| + |O_c|} \sum_{i \in R_c \cup O_c} \text{dist}_{\text{network}}(i, G_{\text{nearest}})$$  Where $R_c$ (residential) and $O_c$ (office/academic) cells are summed, and $\text{dist}_{\text{network}}(i, G_{\text{nearest}})$ is the network distance from cell $i$ to its nearest green space cell.

C. Handling Heterogeneous Objectives: Normalization Strategies

The objectives are heterogeneous: $f_1$ (dollars), $f_2$ (penalty points), $f_3$ (unitless ratio), and $f_4, f_5$ (meters). They must be normalized to be comparable.
	•	Min-Max Normalization:
	•	Formula: $f_i^{\text{norm}}(x) = \frac{f_i(x) - f_i^{\min}}{f_i^{\max} - f_i^{\min}}$ 16
	•	Pros: Binds all objectives to the range .16
	•	Cons: Extremely sensitive to outliers.16 A single infeasible layout with an extreme cost will skew $f^{\max}$, compressing all other solutions near 0 and destroying selection pressure.17
	•	Z-Score Normalization:
	•	Formula: $f_i^{\text{norm}}(x) = \frac{f_i(x) - \mu_i}{\sigma_i}$ 16
	•	Pros: Robust to outliers, as the standard deviation $\sigma_i$ absorbs their impact.16
	•	Cons: Produces an unbounded range, which is unsuitable for algorithms like NSGA-III and MOEA/D that operate on a normalized simplex.18
These standard methods present a problem, but the algorithms themselves provide a solution. NSGA-III performs normalization internally as a core part of its selection mechanism.18 It dynamically finds the ideal point ($z^*$) and nadir point ($z^{\text{nad}}$) from the current population ($S_t$) each generation and normalizes solutions based on these adaptive bounds.18 This makes it inherently robust to outliers.
MOEA/D, by contrast, is highly sensitive to scaling. Its Tchebycheff function, $g^{te} = \max_i \{ \lambda_i \cdot |f_i(x) - z_i^*| \}$ 19, will be dominated by the objective with the largest numerical range, regardless of the weight vector $\lambda$. Therefore, MOEA/D requires explicit, external normalization, whereas NSGA-III handles it internally.
Table 1: Normalization Strategy Comparison

Aspect
Min-Max Normalization
Z-Score Normalization
NSGA-III Internal
Formula
$(x - \min) / (\max - \min)$
$(x - \mu) / \sigma$
$(x - z^*) / (z^{\text{nad}} - z^*)$
Range

Unbounded (typically ~[-3, 3])
(or slightly > 1)
Outlier Sensitivity
Very High 16
Low 16
Low (Adaptive)
Use Case
Data is bounded, prep for MOEA/D.
Data has extreme outliers.
Used internally by NSGA-III

D. Anticipated Pareto Front Characteristics

The geometry of the Pareto front dictates algorithm performance.
	•	Shape: The trade-offs in spatial planning are non-linear. The $f_1$ (Cost) vs. $f_3$ (Compactness) trade-off is likely convex (diminishing returns). The $f_3$ (Compactness) vs. $f_5$ (Green Access) trade-off is inherently concave 10; they are in direct conflict. Real-world problems often have mixed convex/concave fronts.20
	•	Connectivity: Because the genotype uses discrete integer values for land use, the objective space will be disconnected.20 Changing one cell from "Residential" to "Park" is a discrete jump, not a continuous slide. This creates "patches" and "gaps" in the Pareto front, a characteristic shared with test problems like DTLZ7.22
This anticipated geometry (mixed-shape and disconnected) is a key diagnostic for algorithm selection. MOEA/D, which relies on fixed, uniform weight vectors 23, is known to struggle when the Pareto front is not a simple, continuous simplex.24 NSGA-III, being dominance-based, is indifferent to the front's shape (convex or concave).25 Its reference-point niching mechanism 26 actively seeks to find at least one solution near each reference direction, making it theoretically more robust for the gappy, disconnected front expected from this problem.

II. Technical Deep Dive: NSGA-III (Non-dominated Sorting Genetic Algorithm III)


A. Core Philosophy: Dominance and Reference Points

NSGA-III 27 was designed to solve the primary failure of NSGA-II in many-objective ($M > 3$) spaces. In high-dimensional objective spaces, the proportion of non-dominated solutions in a population approaches 100%, rendering the dominance-ranking (Front 1 vs. Front 2) ineffective.26 This leaves NSGA-II to rely solely on crowding distance, which is computationally expensive and inaccurate in $M>3$ dimensions. NSGA-III replaces the crowding distance operator with a more structured reference-point-based niching mechanism.26

B. Reference Point Generation (Das-Dennis Method)

This mechanism creates the structured set of reference points that guide selection.
	•	Mathematical Formulation: The method 29 generates points $\mathbf{Z}$ on a normalized hyperplane $x_1 + x_2 +... + x_M = 1$.30 This is achieved by creating a simplex-lattice design. A parameter $p$ (number of divisions per objective) is chosen. Each reference point $\mathbf{z} = (z_1,..., z_M)$ is constructed such that each $z_i \in \{0, \frac{1}{p}, \frac{2}{p},..., 1\}$ and $\sum_{i=1}^M z_i = 1$.
	•	Number of Points (H): The total number of reference points $H$ generated is given by the binomial coefficient 32:  $$H = \binom{M + p - 1}{p} = \binom{M + p - 1}{M - 1}$$
	•	Population Size Implication: The NSGA-III selection mechanism is designed to find and preserve one solution for each reference point.28 Therefore, the population size $N$ must be approximately equal to $H$.33 $N$ is typically set to the smallest multiple of 4 that is greater than or equal to $H$. This relationship dictates the minimum population size. For our $M=5$ objective problem:
	•	$p=4 \implies H = \binom{5+4-1}{4} = 70$. (Minimum $N=72$)
	•	$p=5 \implies H = \binom{5+5-1}{5} = 126$. (Minimum $N=128$)
	•	$p=6 \implies H = \binom{5+6-1}{6} = 210$. (Minimum $N=212$) A population size $N \ge 100$ is reasonable, implying $p \ge 5$ is required. A value of $p=6$ (yielding $N=212$) provides a more robust sample size.35
	•	pymoo Implementation 28: Python from pymoo.util.ref_dirs import get_reference_directions  # M=5 objectives, p=6 divisions # This creates H = C(5+6-1, 6) = 210 reference directions ref_dirs = get_reference_directions("das-dennis", 5, n_partitions=6) 

C. The NSGA-III Selection Mechanism

This is the main loop for selecting $N$ survivors from the combined parent ($P_t$) and offspring ($Q_t$) population $R_t$ (size 2N).
Pseudocode for NSGA-III Survival Selection
(Based on 18)



Algorithm: NSGA-III Survival Selection (R_t, N, ref_dirs) Input: R_t (combined population, size 2N), N (target pop size), ref_dirs (H reference directions) Output: P_{t+1} (surviving population, size N)  1.  (F_1, F_2,...) <- Non-Dominated-Sort(R_t) 2.  P_{t+1} <- ∅ 3.  i <- 1 4.  while |P_{t+1}| + |F_i| <= N: 5.      P_{t+1} <- P_{t+1} ∪ F_i    // Add all solutions from "good" fronts 6.      i <- i + 1 7.  F_l <- F_i                     // F_l is the "last front" that does not fit 8.  S_t <- P_{t+1} ∪ F_l             // All solutions from fronts F_1 to F_l 9.   10. // --- Normalization  --- 11. z_ideal <- FindIdealPoint(S_t)      // z_i = min(f_i) for all s in S_t 12. z_nadir <- FindNadirPoint(S_t)      // (See note below) 13. S_t_norm <- Normalize(S_t, z_ideal, z_nadir) 14.  15. // --- Association [26, 37] --- 16. for s in S_t_norm: 17.     r_assoc[s] <- FindNearestRefPoint(s, ref_dirs) // By perpendicular distance 18.  19. // --- Niche Count [29] --- 20. for j in 1 to |ref_dirs|: 21.     rho_j <- 0                     // Niche count for ref_dir j 22.     for s in P_{t+1}:              // Count solutions from F_1 to F_{l-1} 23.         if r_assoc[s] == j: 24.             rho_j <- rho_j + 1 25. 26. // --- Niche Preservation [28, 29] --- 27. K <- N - |P_{t+1}| // Number of individuals to choose from F_l 28. for k in 1 to K: 29.     J_min <- {j | argmin_j(rho_j)} // Set of ref_dirs with min count 30.     j_star <- RandomlySelect(J_min) 31.  32.     Candidates <- {s | s in F_l and r_assoc[s] == j_star} 33.  34.     if |Candidates| > 0: 35.         if rho_j_star == 0:       // Scenario A: ref_dir has no members [29] 36.             s_next <- FindClosest(Candidates, j_star) // Closest by perp. dist 37.         else:                   // Scenario B: ref_dir has members [29] 38.             s_next <- RandomlySelect(Candidates)  39. 40.         P_{t+1} <- P_{t+1} ∪ {s_next} 41.         F_l <- F_l \ {s_next} 42.         rho_j_star <- rho_j_star + 1 43.     else: 44.         // No solution in F_l is associated with j_star 45.         rho_j_star <- infinity   // Exclude this ref_dir from now on 46.  47. return P_{t+1} 
The adaptive nadir point ($z^{\text{nad}}$, Line 12) is critical to the internal normalization. It is not a simple maximum of the population. Instead, it is found by first identifying $M$ "extreme points" from $S_t$, where each extreme point is the solution that performs best along one of the objective axes (often found using an Achievement Scalarization Function). A hyperplane is constructed from these $M$ points, and $z^{\text{nad}}$ is defined by the intercepts of this hyperplane with the objective axes.18

D. Computational Complexity

The worst-case complexity of one generation of NSGA-III is analyzed by summing its components 31:
	•	Non-Dominated Sort (Line 1): Sorting $2N$ individuals takes $O(M(2N)^2) = O(MN^2)$.31
	•	Normalization (Lines 11-13): Finding the ideal point is $O(MN)$. Finding the $M$ extreme points for the nadir point is $O(M^2N)$.31
	•	Association (Lines 16-17): Associating $2N$ solutions with $H$ reference points (each $M$-dimensional) takes $O(M \cdot N \cdot H)$.31
	•	Niching (Lines 26-45): This loop runs $K$ times ($K \le N$). The inner steps are at worst $O(L)$ or $O(H)$, where $L = |F_l| \le 2N$. The total is $O(N \cdot (H+L)) \approx O(N(H+N))$.
Since $N \approx H$ 34, the complexities become $O(MN^2)$, $O(M^2N)$ [Nadir], $O(MN^2)$ [Association], and $O(N^2)$ [Niching]. The overall complexity is the maximum of these, which is $\max(O(MN^2), O(M^2N))$. For most many-objective problems, $N > M$, making the $O(MN^2)$ term dominant. This aligns with multiple sources.33

E. Implementation Guide with pymoo

The following provides the end-to-end pymoo implementation.27
	•	Step 1: Define the Custom Problem This class defines the 5-objective problem. For complex genotypes like a grid, it is standard to use n_var=1 and a dtype=object. Python import numpy as np from pymoo.core.problem import ElementwiseProblem  # Assume 'calculate_objectives' is a user-defined function # that takes a layout object and returns a tuple of 5 objective values from your_spatial_functions import calculate_objectives, Layout  class CampusProblem(ElementwiseProblem):     def __init__(self, grid_size=(20, 20)):         self.grid_size = grid_size         # We use n_var=1, where that one variable is a complex 'Layout' object         super().__init__(n_var=1, n_obj=5, n_ieq_constr=0, vtype=object)       def _evaluate(self, x, out, *args, **kwargs):         # 'x' is a 1D array containing a single element: the layout object         layout_genotype = x           # Calculate the 5 objectives         cost, incomp, non_compact, walk, green = calculate_objectives(layout_genotype)          # pymoo minimizes by default         out["F"] = [cost, incomp, non_compact, walk, green]  # Note: Custom sampling, crossover, and mutation operators are # required for this problem definition (See Section IV) 
	•	Step 2: Initialize and Run NSGA-III 28 Python from pymoo.algorithms.moo.nsga3 import NSGA3 from pymoo.optimize import minimize from pymoo.util.ref_dirs import get_reference_directions from pymoo.termination import get_termination  # Import custom operators (defined in Section IV.C) from custom_operators import SpatialSampling, SpatialCrossover, SpatialMutation  # 1. Create the problem instance problem = CampusProblem()  # 2. Setup the Reference Directions (M=5, p=6 => H=210) ref_dirs = get_reference_directions("das-dennis", 5, n_partitions=6)  # 3. Setup the Algorithm  #    Population size N must be ~H. 212 is the next multiple of 4 >= 210. algorithm = NSGA3(     pop_size=212,     ref_dirs=ref_dirs,     sampling=SpatialSampling(),     crossover=SpatialCrossover(prob=0.9),     mutation=SpatialMutation(prob=0.1),     eliminate_duplicates=True  # Good for discrete problems )  # 4. Define Termination (e.g., 500 generations) termination = get_termination("n_gen", 500)  # 5. Run the optimization res = minimize(     problem,     algorithm,     termination,     seed=1,     save_history=True,     verbose=True )  # 6. Access results print("Objective Values (Pareto Front): \n", res.F) print("Decision Variables (Layouts): \n", res.X) 

III. Technical Deep Dive: MOEA/D (Multi-Objective Evolutionary Algorithm based on Decomposition)


A. Core Philosophy: Decomposition and Scalarization

MOEA/D (Multi-Objective Evolutionary Algorithm based on Decomposition) 19 is a fundamentally different approach. Instead of using Pareto dominance, it decomposes the MOP into $N$ scalar (single-objective) subproblems and optimizes them simultaneously.41 Each subproblem $j$ is defined by a weight vector $\lambda^j = (\lambda_1^j,..., \lambda_M^j)$ and a scalarization function $g(\mathbf{x} | \lambda^j, \mathbf{z}^*)$. The algorithm maintains a population of $N$ solutions, where $x^j$ is the current best solution found for subproblem $j$.

B. Weight Vector Generation

This is analogous to NSGA-III's reference point generation, and the same methods are used.
	•	Method: The standard method is the simplex-lattice design 23, which generates a set of uniformly distributed weight vectors.32
	•	Weakness: This fixed, uniform set of weights is a known weakness.23 If the Pareto front is biased (e.g., DTLZ4) or disconnected (as our spatial problem is), MOEA/D's performance degrades. Weights in "gappy" regions find no solutions, while weights in "dense" regions are all pulled to the same few solutions.24
	•	pymoo Implementation 43: Python from pymoo.util.ref_dirs import get_reference_directions # M=5 objectives, p=6 divisions => N=210 weight vectors ref_dirs = get_reference_directions("uniform", 5, n_partitions=6) 

C. Scalarization Functions: The Tchebycheff Approach

This function $g(\cdot)$ converts the $M$-objective vector $\mathbf{f}(\mathbf{x})$ into a single fitness value.
	•	Mathematical Formulation (Minimization): The Tchebycheff (or Chebyshev) scalarization is most common.19 $$g^{te}(\mathbf{x} | \lambda^j, \mathbf{z}^*) = \max_{1 \le i \le M} \{ \lambda_i^j \cdot |f_i(\mathbf{x}) - z_i^*| \}$$  Where:
	•	$\mathbf{x}$ is the candidate solution.
	•	$\lambda^j$ is the weight vector for subproblem $j$.
	•	$\mathbf{z}^*$ is the ideal point (the vector of best-seen values for each objective), which is updated dynamically.19
	•	The simpler "Weighted Sum" method ($g^{ws} = \sum \lambda_i^j \cdot f_i(\mathbf{x})$) is incapable of finding solutions on non-convex (concave) portions of a Pareto front. The Tchebycheff function can handle non-convex fronts.41 Given our problem's anticipated concave trade-offs (Section I.D), Tchebycheff is the correct choice.

D. Neighborhood-Based Mating and Update

This is the core "information sharing" mechanism of MOEA/D.
	•	Neighborhood Definition ($B(i)$): For each subproblem $i$, its neighborhood $B(i)$ is defined as the set of $T$ subproblems whose weight vectors ($\lambda^j$) are closest to $\lambda^i$ (based on Euclidean distance).32 $T$ is a crucial, user-defined parameter.
	•	Mating Selection: To generate a new candidate solution $y$ for subproblem $i$, parents are selected only from the current solutions $\{x^j | j \in B(i)\}$.45 This enforces local mating.
	•	Update Procedure: After a new solution $y$ is generated, it is evaluated against the $T$ subproblems in its neighborhood.
Pseudocode for MOEA/D Main Loop
(Based on 19)



Algorithm: MOEA/D Main Loop 1.  Initialize N weight vectors {λ^1,..., λ^N} (e.g., Das-Dennis) 2.  Initialize population P = {x^1,..., x^N} (randomly) 3.  Initialize ideal point z* 4.  Determine neighborhood B(i) for each i (e.g., T nearest neighbors) 5. 6.  while (not terminated): 7.      for i in 1 to N: // Iterate through all subproblems 8.          // 1. Mating Selection (from neighborhood) 9.          Parents <- SelectParents(P, B(i)) 10.         // 2. Reproduction (using custom spatial operators) 11.         y <- CrossoverAndMutation(Parents) 12.         // 3. Update Ideal Point z* 13.         z* <- UpdateIdeal(z*, f(y)) 14.         // 4. Update Neighborhood Solutions  15.         for j in B(i): 16.             if g^{te}(y | λ^j, z*) < g^{te}(x^j | λ^j, z*): 17.                 x^j = y // Replace neighbor's solution 
	•	Diversity Risk: The update step (Line 14-17) is extremely aggressive. A single "good" solution $y$ can instantly replace $T$ other solutions. This aggressive replacement "will destroy the diversity of the population" and can lead to premature convergence in a local optimum.46 This makes $T$ a critical trade-off:
	•	Small $T$: Slower convergence, better population diversity.45
	•	Large $T$: Faster convergence, high risk of diversity loss.45

E. Computational Complexity

	•	The main loop (Line 7) runs $N$ times per generation.
	•	The dominant cost is the update loop (Line 15), which runs $T$ times.
	•	Calculating $g^{te}$ (Line 16) takes $O(M)$ time.
	•	Therefore, the complexity per generation is $O(N \cdot T \cdot M) = O(MNT)$.
	•	If $M$ and $T$ are treated as constants, this is $O(N)$. This is significantly more efficient per generation than NSGA-III's $O(MN^2)$.19

F. Implementation Guide with pymoo

	•	Step 1: Define the Custom Problem: Use the exact same CampusProblem class from Section II.E.
	•	Step 2: Initialize and Run MOEA/D 43 Python from pymoo.algorithms.moo.moead import MOEAD from pymoo.decomposition.tchebicheff import Tchebicheff from pymoo.optimize import minimize from pymoo.util.ref_dirs import get_reference_directions from pymoo.termination import get_termination  # Import custom operators from custom_operators import SpatialSampling, SpatialCrossover, SpatialMutation  # 1. Create the problem instance problem = CampusProblem()  # 2. Setup the Weight Vectors (M=5, p=6 => N=210) ref_dirs = get_reference_directions("uniform", 5, n_partitions=6)  # 3. Setup the Algorithm  algorithm = MOEAD(     ref_dirs,     n_neighbors=20,          # T=20, a common starting value     decomposition=Tchebicheff(), # Explicitly use Tchebycheff     prob_neighbor_mating=0.9,  # 90% chance to mate within neighborhood     sampling=SpatialSampling(),     crossover=SpatialCrossover(prob=0.9),     mutation=SpatialMutation(prob=0.1) )  # 4. Define Termination termination = get_termination("n_gen", 500)  # 5. Run the optimization res = minimize(     problem,     algorithm,     termination,     seed=1,     save_history=True,     verbose=True ) 

IV. Customizing Genetic Operators for Spatial Problems

Standard pymoo operators like Simulated Binary Crossover (SBX) 48 and Polynomial Mutation (PM) 49 are designed for continuous or simple integer vectors. They will fail on a complex layout object. Custom operators must be implemented.50

A. Spatial Crossover Operators

	•	Two-Point Tiling Crossover: Select a random row and column. This divides both parent grids (A, B) into four quadrants. Child 1 is created by taking quadrants 1 and 3 from Parent A and 2 and 4 from Parent B. Child 2 is the inverse. This maintains spatial locality.
	•	Common Point Crossover 52: (Suited for graph representations). Find common nodes (e.g., "Library" building) in both parent layouts. Select one as a crossover point and swap the sub-graphs/sub-paths connected to it.
	•	Voronoi Crossover: Randomly scatter $k$ "seed" points on the grid. Assign land-use types to seeds by alternating between Parent A and Parent B. The child's grid is generated by a Voronoi tessellation, where each cell $(x,y)$ gets the land-use type of the nearest seed point. This creates novel, spatially coherent children.

B. Spatial Mutation Operators

	•	Point Mutation (Node-Label Mutation) 1: Select one or more grid cells at random. Change their land-use type to a new, randomly chosen valid type. This is the simplest way to introduce new genetic material.
	•	Sub-Path Replacement 52: (Targets network objectives). Identify a path in the layout. Delete a random segment of it. Re-connect the two endpoints using a new path (e.g., a random walk or A* search).52
	•	Swap Mutation (Swap-Node) 1: Select two random grid cells of different land-use types. Swap their types. This is a very useful operator as it maintains the exact global count of each land-use type, which is often a problem constraint.

C. Implementation in pymoo
51

The following snippets provide the class skeletons for implementing these custom operators, inheriting from the correct pymoo.core base classes.51

Python


from pymoo.core.sampling import Sampling from pymoo.core.crossover import Crossover from pymoo.core.mutation import Mutation import random import numpy as np  # Assume 'Layout' is a custom class that holds the grid from your_spatial_functions import Layout, get_random_type  class SpatialSampling(Sampling):     """     Generates the initial population of N random layouts.     """     def _do(self, problem, n_samples, **kwargs):         X = np.full((n_samples, 1), None, dtype=object)         for i in range(n_samples):             # Create a 20x20 layout with random land uses             X[i, 0] = Layout(problem.grid_size, random_init=True)         return X  class SpatialCrossover(Crossover):     """     Implements a Two-Point Tiling Crossover.     """     def __init__(self, prob=0.9):         # 2 parents, 2 offspring         super().__init__(n_parents=2, n_offsprings=2, prob=prob)      def _do(self, problem, X, **kwargs):         # X is a (n_samples, n_parents, n_var) array         p1_layouts = X[:, 0, 0]         p2_layouts = X[:, 1, 0]                  # Offspring array         offs = np.full((len(p1_layouts), self.n_offsprings), None, dtype=object)          for i in range(len(p1_layouts)):             parent1 = p1_layouts[i]             parent2 = p2_layouts[i]                          # Perform custom crossover logic             child1_grid, child2_grid = self.tiling_crossover(parent1.grid, parent2.grid)                          offs[i, 0] = Layout(grid_data=child1_grid)             offs[i, 1] = Layout(grid_data=child2_grid)                      return offs      def tiling_crossover(self, grid1, grid2):         #... (Implementation logic for 2-point tiling crossover)...         # (Returns two new grid numpy arrays)         new_grid1 = grid1.copy() # Placeholder         new_grid2 = grid2.copy() # Placeholder         return new_grid1, new_grid2  class SpatialMutation(Mutation):     """     Implements a Point Mutation.     """     def __init__(self, prob=0.1):         super().__init__(prob=prob)      def _do(self, problem, X, **kwargs):         # X is (n_samples, n_var) array of layouts         for i in range(len(X)):             if random.random() < self.prob:                 # Perform Point Mutation                 mutated_grid = self.point_mutation(X[i, 0].grid)                 X[i, 0] = Layout(grid_data=mutated_grid)         return X              def point_mutation(self, grid):         #... (Implementation logic to change one cell's type)...         new_grid = grid.copy() # Placeholder         return new_grid 

V. Benchmarking, Metrics, and Performance Analysis

This section defines the methodology for evaluating the performance of NSGA-III and MOEA/D.

A. Standard Benchmark Suites (DTLZ and WFG)

Before running on the complex campus problem, implementations must be validated on standard benchmarks.53 The DTLZ 54 and WFG 55 suites are designed to be diagnostic.22 The key is to select benchmarks that mimic the anticipated characteristics of our spatial problem (disconnected, biased, and mixed-shape fronts).
Table 2: Key Diagnostic Benchmarks for Spatial Planning (M=5)
Problem
PF Shape
Modality
Connectivity
Key Challenge Tested [54, 55]
DTLZ1
Linear
Multimodal
Connected
Convergence (avoiding $11^k-1$ local optima)
DTLZ2
Concave
Unimodal
Connected
Baseline Scalability to M=5
DTLZ4
Concave
Unimodal
Connected
Biased Distribution (tests diversity on non-uniform fronts)
DTLZ7
Mixed
Multimodal
Disconnected
Maintain solutions in $2^{M-1}$ = 16 distinct regions
WFG2
Convex
Unimodal
Disconnected
Non-separable & Disconnected
WFG5
Concave
Deceptive
Connected
Deceptiveness (avoiding false, misleading optima)

B. Performance Indicators

Quantitative metrics are required to compare the resulting Pareto fronts.57
	•	Hypervolume (HV)
	•	Formula: $HV(S, \mathbf{z}^{\text{ref}}) = \text{Volume}(\bigcup_{s \in S} \text{hypercube}(s, \mathbf{z}^{\text{ref}}))$.58
	•	Interpretation: Measures the volume of objective space (in M=5 dimensions) dominated by the solution set $S$, bounded by a reference point $\mathbf{z}^{\text{ref}}$.58
	•	Pros: The only strictly Pareto-compliant metric; a single value measuring both convergence and diversity.59
	•	Cons: Computationally NP-hard.59 For $M=5$, exact calculation is extremely slow. A Monte Carlo approximation is used (available in pymoo 61).
	•	Inverted Generational Distance Plus (IGD+)
	•	Formula: $IGD^+(S, R) = \frac{1}{|R|} \sum_{r \in R} \min_{s \in S} d^+(r, s)$.62
	•	Where $R$ is a large set of points sampled from the true Pareto front, and $d^+(r, s) = \sqrt{\sum (\max\{r_k - s_k, 0\})^2}$ is a modified distance.62
	•	Interpretation: Measures the average distance from points on the true front to the nearest solution in the obtained set $S$.
	•	Note: Standard IGD is not Pareto-compliant.64 IGD+ is weakly Pareto-compliant and must be used instead.62 This metric requires the true Pareto front, limiting it to benchmark problems.
	•	Spacing Metric (SP)
	•	Formula: $SP(S) = \sqrt{ \frac{1}{|S|-1} \sum_{i=1}^{|S|} (d_i - \bar{d})^2 }$.65
	•	Where $d_i$ is the distance to the nearest neighbor of solution $i$ in the set $S$, and $\bar{d}$ is the mean of $d_i$.
	•	Interpretation: Measures the uniformity of the solution set.65 A value near 0 is ideal. It does not measure convergence.
	•	Computational Time
	•	A simple wall-clock measurement of the time required to meet the termination criterion.66

C. Parameter Tuning and Termination Criteria

	•	Population Size (N): As derived in Section II.B, $N$ is not an arbitrary parameter but is dictated by $M$ and $p$ (divisions).33 For $M=5$, $N \ge 100$ is a minimum (requiring $p \approx 5$). A more robust starting point is $p=6$ (for $N=210$, used as $N=212$) or $p=7$ ($N=252$).35 This applies to both NSGA-III and MOEA/D.
	•	Crossover/Mutation Rates: For genetic operators, a high crossover probability is standard (e.g., $p_c = 0.9$).69 A standard mutation rate is $p_m = 1/D$, where $D$ is the number of decision variables (e.g., $D = 20 \times 20 = 400$ grid cells), yielding $p_m \approx 0.0025$.
	•	Termination Criteria:
	•	Method 1: Max Generations. Simple (e.g., n_gen=500), but arbitrary and inefficient.60
	•	Method 2: Convergence-Based. Strongly recommended. pymoo implements a "Running Metric" that tracks generation-to-generation improvement and stops when the front stabilizes.70 This is far superior as it adapts to problem difficulty.70
	•	pymoo Convergence-Based Termination 70: Python from pymoo.termination.default import DefaultMultiObjectiveTermination  # This termination tracks the change in the front's properties # over a 'period' of generations and stops when the change # is less than a tolerance 'ftol'. termination = DefaultMultiObjectiveTermination(     xtol=1e-8,       # Tolerance in decision space     cvtol=1e-6,      # Tolerance in constraint violation     ftol=0.0025,     # Tolerance in objective space     period=30,       # Number of generations to check for change     n_skip=10,       # Skip first 10 generations     n_max_gen=1000,  # Hard stop at 1000 generations     n_max_evals=100000 # Hard stop at 100,000 evaluations ) 

VI. Comparative Analysis and Recommendations for Campus Planning


A. NSGA-III vs. MOEA/D: A Head-to-Head Comparison

The two algorithms represent distinct philosophies for many-objective optimization.
Table 3: Summary of Algorithmic Differences

Feature
NSGA-III
MOEA/D
Core Idea
Dominance-based Sorting + Reference Point Niching 26
Decomposition into N Single-Objective Subproblems 24
Selection
Elitist Non-Dominated Sort 18
Scalarization Fitness (e.g., Tchebycheff) 19
Diversity
Niche-preservation operator [29]
Neighborhood-based mating & replacement 45
Complexity
$O(MN^2)$ 31 - Dominated by sort & association
$O(MNT)$ 47 - Dominated by neighborhood update
PF Handling
Excellent. Indifferent to shape (convex, concave) and robust to disconnected fronts.25
Good, but... Good for non-convex 41, but struggles with biased 31 and disconnected 24 fronts.
Key Parameter
$p$ (divisions, which sets $N \approx H$) 34
$T$ (neighborhood size) 45
Normalization
Internal. Adaptive normalization is a core part of the algorithm.18
External. Requires explicit normalization for heterogeneous objectives.

B. Synthesis of Performance on Benchmarks

Literature provides a clear diagnostic picture of how these algorithms perform under stress.
Table 4: Synthesis of Literature Benchmark Performance (M $\ge$ 5)

Problem
Metric
Winner / Observation
Source(s)
DTLZ1 (Multimodal)
IGD
NSGA-III (or variants) often better than MOEA/D.
[72]
DTLZ2-3 (Concave)
IGD
MOEA/D-PBI can be superior, but NSGA-III is highly consistent.
[31, 72]
DTLZ4 (Biased)
IGD
NSGA-III is a clear winner. MOEA/D fails to maintain distribution.
31
WFG Suite (General)
HV, IGD
NSGA-III is highly competitive, benefits from internal normalization.
[73, 74]
Real-World (Lake)
GD, HV
Both algorithms performed poorly. Highly sensitive to parameters.
75

C. Final Recommendations for Campus Planning

	•	Primary Recommendation: Start with NSGA-III.
	•	Reasoning: The 5-objective campus planning problem, due to its discrete nature and complex trade-offs, is expected to have a disconnected and biased Pareto front (Section I.D). The benchmark analysis provides a direct diagnostic for this: MOEA/D's performance collapses on biased fronts like DTLZ4 31, as its fixed, uniform weight vectors cannot adapt. NSGA-III's reference-point niching 26 and dominance-based approach 25 are fundamentally more robust, designed to maintain solutions across gappy and non-uniform fronts. Furthermore, its internal adaptive normalization 18 automatically handles the heterogeneous objectives.
	•	Parameters: Use $M=5$, $p=6$ or $p=7$ (giving $N \approx 212-252$), and the convergence-based termination criterion.70
	•	Secondary Recommendation: Use MOEA/D for Speed, with Caution.
	•	Reasoning: If the $O(MN^2)$ complexity of NSGA-III proves too slow, MOEA/D's $O(MNT)$ complexity makes it a faster alternative.47
	•	Cautions:
	•	It must be paired with a robust, dynamic, external normalization strategy (Section I.C).
	•	It requires careful tuning of the neighborhood size $T$.45
	•	It may fail to find solutions in biased or gappy regions of the trade-off space.31
	•	Based on literature, the PBI (Penalty-Based Boundary Intersection) decomposition often outperforms Tchebycheff on DTLZ problems and should be considered.31
	•	Final Expert Suggestion (Beyond the Query): The struggles of both algorithms on complex real-world problems like the "Lake Problem" 75 highlight that fixed reference points/weights are a core weakness. The state-of-the-art solution is to use an adaptive algorithm. For a robust solution to this spatial planning problem, an investigation of A-NSGA-III (Adaptive NSGA-III) 26 or MOEA/D-AWA (MOEA/D with Adaptive Weight Adjustment) 24 is strongly recommended. These algorithms adapt their reference vectors during the run to match the problem's actual Pareto front shape, combining search power with the adaptability required for complex, real-world problems.
Alıntılanan çalışmalar
	•	A graph based evolutionary algorithm - SciSpace, erişim tarihi Kasım 3, 2025, https://scispace.com/pdf/a-graph-based-evolutionary-algorithm-6qd1psh3pj.pdf
	•	Research on Multi-Objective Optimization Design of University Student Center in China Based on Low Energy Consumption and Thermal Comfort - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/1996-1073/17/9/2082
	•	Sustainable and Resilient Land Use Planning: A Multi-Objective ..., erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2220-9964/13/3/99
	•	(PDF) Multi-Objective Optimization Methods for University Campus Planning and Design—A Case Study of Dalian University of Technology - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/393868738_Multi-Objective_Optimization_Methods_for_University_Campus_Planning_and_Design-A_Case_Study_of_Dalian_University_of_Technology
	•	Multi-Objective Optimization Methods for University Campus Planning and Design—A Case Study of Dalian University of Technology - DOAJ, erişim tarihi Kasım 3, 2025, https://doaj.org/article/3dcd5f7e88734e45926c166c815984e1
	•	A 3D Model for Optimizing Infrastructure Costs in Road Design - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/323232783_A_3D_Model_for_Optimizing_Infrastructure_Costs_in_Road_Design
	•	Operations Benefit/Cost Analysis Desk Reference - Chapter 2 Overview of B/C Analysis for Operations - FHWA Office of Operations, erişim tarihi Kasım 3, 2025, https://ops.fhwa.dot.gov/publications/fhwahop12028/sec2.htm
	•	Middle East Property and Construction Handbook - Balancing the cost of infrastructure - masterplan cost estimation - aecom, erişim tarihi Kasım 3, 2025, http://publications.aecom.com/MEH/article/balancing-the-cost-of-infrastructure-masterplan-cost-estimation/
	•	(PDF) An Evolutionary Multi-Architecture Multi-Objective Optimization Algorithm for Design Space Exploration - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/289503652_An_Evolutionary_Multi-Architecture_Multi-Objective_Optimization_Algorithm_for_Design_Space_Exploration
	•	Green and Compact: A Spatial Planning Model for Knowledge-Based Urban Development in Peri-Urban Areas - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2071-1050/13/23/13365
	•	Spatial Configurations and Walkability Potentials. Measuring Urban Compactness with Space Syntax - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/351819893_Spatial_Configurations_and_Walkability_Potentials_Measuring_Urban_Compactness_with_Space_Syntax
	•	Human Settlements, Infrastructure, and Spatial Planning - IPCC, erişim tarihi Kasım 3, 2025, https://www.ipcc.ch/site/assets/uploads/2018/02/ipcc_wg3_ar5_chapter12.pdf
	•	Spatial Pattern of the Walkability Index, Walk Score and Walk Score Modification for Elderly - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2220-9964/11/5/279
	•	Full article: Generative urban modelling for walkable neighbourhoods: design optimization for pedestrian-oriented street networks - Taylor & Francis Online, erişim tarihi Kasım 3, 2025, https://www.tandfonline.com/doi/full/10.1080/13574809.2025.2517652
	•	Comfort and Time-Based Walkability Index Design: A GIS-Based Proposal - PMC, erişim tarihi Kasım 3, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC6719924/
	•	Min-Max and Z-Score Normalization | Codecademy, erişim tarihi Kasım 3, 2025, https://www.codecademy.com/article/min-max-zscore-normalization
	•	machine learning - z-score VS min-max normalization - Cross ..., erişim tarihi Kasım 3, 2025, https://stats.stackexchange.com/questions/547446/z-score-vs-min-max-normalization
	•	Investigating the Normalization Procedure of NSGA-III - MSU College of Engineering, erişim tarihi Kasım 3, 2025, https://www.egr.msu.edu/~kdeb/papers/c2018009.pdf
	•	MOEA/D: A Multiobjective Evolutionary Algorithm Based on ..., erişim tarihi Kasım 3, 2025, https://faculty.csu.edu.cn/_resources/group1/M00/00/65/wKiylWHsB8eABncgABjeCL_K05s052.pdf
	•	Different geometrical shapes of Pareto fronts. a The Pareto front of... | Download Scientific Diagram - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/figure/Different-geometrical-shapes-of-Pareto-fronts-a-The-Pareto-front-of-UF1-UF2-UF3-which_fig12_319715838
	•	arXiv:2009.12867v1 [cs.NE] 27 Sep 2020, erişim tarihi Kasım 3, 2025, https://arxiv.org/pdf/2009.12867
	•	Test Problems for Large-Scale Multiobjective and Many-Objective Optimization - Soft Computing, erişim tarihi Kasım 3, 2025, http://www.soft-computing.de/LSMOP_Final.pdf
	•	(PDF) Weight Vector Definition for MOEA/D-Based Algorithms Using Augmented Covering Arrays for Many-Objective Optimization - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/380933964_Weight_Vector_Definition_for_MOEAD-Based_Algorithms_Using_Augmented_Covering_Arrays_for_Many-Objective_Optimization
	•	MOEA/D with adaptive weight adjustment - PubMed, erişim tarihi Kasım 3, 2025, https://pubmed.ncbi.nlm.nih.gov/23777254/
	•	Visualization of Pareto Front Points when Solving Multi-objective Optimization Problems - SciSpace, erişim tarihi Kasım 3, 2025, https://scispace.com/pdf/visualization-of-pareto-front-points-when-solving-multi-2ftfzcaf7p.pdf
	•	A Non-Dominated Sorting Evolutionary Algorithm Updating When Required - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/html/2507.03864v1
	•	pymoo: Multi-objective Optimization in Python, erişim tarihi Kasım 3, 2025, https://pymoo.org/
	•	NSGA-III - pymoo, erişim tarihi Kasım 3, 2025, https://pymoo.org/algorithms/moo/nsga3.html
	•	Many-Objective Software Remodularization using NSGA-III - arXiv, erişim tarihi Kasım 3, 2025, https://arxiv.org/pdf/2005.06510
	•	EF1-NSGA-III: An Evolutionary Algorithm Based on the First Front to Obtain Non-Negative and Non-Repeated Extreme Points - Redalyc, erişim tarihi Kasım 3, 2025, https://www.redalyc.org/journal/643/64379866006/html/
	•	An Evolutionary Many-Objective Optimization Algorithm Using ..., erişim tarihi Kasım 3, 2025, https://www.egr.msu.edu/~kdeb/papers/k2012009.pdf
	•	The MOEADr Package: A Component-Based Framework for ..., erişim tarihi Kasım 3, 2025, https://www.jstatsoft.org/article/view/v092i06/1337
	•	U-NSGA-III: A Unified Evolutionary Algorithm for Single, Multiple, and Many-Objective Optimization - MSU College of Engineering, erişim tarihi Kasım 3, 2025, https://www.egr.msu.edu/~kdeb/papers/c2014022.pdf
	•	The Impact of Population Size, Number of Children, and Number of Reference Points on The Performance of NSGA-III - Ryoji Tanabe, erişim tarihi Kasım 3, 2025, https://ryojitanabe.github.io/pdf/to-emo2017.pdf
	•	Setting of population size in NSGA-III, MOEA/D, θ-DEA and RVEA, where p... | Download Table - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/figure/Setting-of-population-size-in-NSGA-III-MOEA-D-th-DEA-and-RVEA-where-p-1-and-p-2-are_tbl2_315781627
	•	Reference Directions - pymoo, erişim tarihi Kasım 3, 2025, https://pymoo.org/misc/reference_directions.html
	•	A niche-elimination operation based NSGA-III algorithm for many-objective optimization, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/317673142_A_niche-elimination_operation_based_NSGA-III_algorithm_for_many-objective_optimization
	•	An Improved NSGA-III with a Comprehensive Adaptive Penalty Scheme for Many-Objective Optimization - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2073-8994/16/10/1289
	•	Part V: Some more useful Information - pymoo, erişim tarihi Kasım 3, 2025, https://pymoo.org/getting_started/part_5.html
	•	Research on Multi-Objective Evolutionary Algorithms Based on Large-Scale Decision Variable Analysis - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2076-3417/14/22/10309
	•	A Generalized Scalarization Method for Evolutionary Multi-Objective Optimization, erişim tarihi Kasım 3, 2025, https://ojs.aaai.org/index.php/AAAI/article/view/26474/26246
	•	Weight Vector Definition for MOEA/D-Based Algorithms Using Augmented Covering Arrays for Many-Objective Optimization - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2227-7390/12/11/1680
	•	MOEA/D - pymoo, erişim tarihi Kasım 3, 2025, https://pymoo.org/algorithms/moo/moead.html
	•	A Study on the Specification of a Scalarizing Function in MOEA/D for Many-Objective Knapsack Problems - Computational Intelligence Labo., erişim tarihi Kasım 3, 2025, https://ci-labo-omu.github.io/assets/paper/pdf_file/multiobjective/Final_LION_2013_MOEA_D.pdf
	•	Relation between Neighborhood Size and MOEA/D Performance on Many-Objective Problems, erişim tarihi Kasım 3, 2025, https://ci-labo-omu.github.io/assets/paper/pdf_file/multiobjective/EMO_2013_MOEA_D_Final.pdf
	•	(PDF) A Modified MOEA/D Algorithm for Solving Bi-Objective Multi-Stage Weapon-Target Assignment Problem - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/351504047_A_Modified_MOEAD_Algorithm_for_Solving_Bi-Objective_Multi-Stage_Weapon-Target_Assignment_Problem
	•	Comparison between MOEA/D and NSGA-III on a set of many and ..., erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/331102607_Comparison_between_MOEAD_and_NSGA-III_on_a_set_of_many_and_multi-objective_benchmark_problems_with_challenging_difficulties
	•	Crossover - pymoo, erişim tarihi Kasım 3, 2025, https://pymoo.org/operators/crossover.html
	•	Operators - pymoo, erişim tarihi Kasım 3, 2025, https://www.pymoo.org/operators/index.html
	•	Pymoo: Multi-Objective Optimization in Python - Julian Blank, erişim tarihi Kasım 3, 2025, https://www.julianblank.com/_static/research/ieee20-pymoo.pdf
	•	Custom Variable Type - pymoo, erişim tarihi Kasım 3, 2025, https://pymoo.org/customization/custom.html
	•	Genetic Algorithms - Crossover and Mutation operators for paths ..., erişim tarihi Kasım 3, 2025, https://stackoverflow.com/questions/12687963/genetic-algorithms-crossover-and-mutation-operators-for-paths
	•	Scalable test problems for evolutionary multi-objective optimization - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/publication/285752813_Scalable_test_problems_for_evolutionary_multi-objective_optimization
	•	DTLZ - pymoo, erişim tarihi Kasım 3, 2025, https://pymoo.org/problems/many/dtlz.html
	•	WFG problem test suite — pagmo 2.19.1 documentation, erişim tarihi Kasım 3, 2025, https://esa.github.io/pagmo2/docs/cpp/problems/wfg.html
	•	A Review of Multi-objective Test Problems and a Scalable Test Problem Toolkit - Edith Cowan University, erişim tarihi Kasım 3, 2025, https://ro.ecu.edu.au/cgi/viewcontent.cgi?article=3021&context=ecuworks
	•	Performance Evaluation Metrics for Multi-Objective Evolutionary Algorithms in Search-Based Software Engineering: Systematic Literature Review - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2076-3417/11/7/3117
	•	A Two-Stage Hypervolume-Based Evolutionary Algorithm for Many ..., erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2227-7390/11/20/4247
	•	The hypervolume indicator for multi-objective optimisation: calculation and use - the UWA Profiles and Research Repository, erişim tarihi Kasım 3, 2025, https://research-repository.uwa.edu.au/files/3236619/Bradstreet_Lucas_2011.pdf
	•	A tutorial on multiobjective optimization: fundamentals and evolutionary methods - NIH, erişim tarihi Kasım 3, 2025, https://pmc.ncbi.nlm.nih.gov/articles/PMC6105305/
	•	How to get hypervolume calculation for Pareto Front in python?, erişim tarihi Kasım 3, 2025, https://or.stackexchange.com/questions/11108/how-to-get-hypervolume-calculation-for-pareto-front-in-python
	•	Inverted Generational Distance (IGD and IGD+) and Averaged ..., erişim tarihi Kasım 3, 2025, https://mlopez-ibanez.github.io/eaf/reference/igd.html
	•	Inverted Generational Distance (IGD and IGD+) and Averaged... - R, erişim tarihi Kasım 3, 2025, https://search.r-project.org/CRAN/refmans/eaf/html/igd.html
	•	An empirical assessment of the properties of inverted generational distance on multi- and many-objective optimization - The University of Manchester, erişim tarihi Kasım 3, 2025, https://pure.manchester.ac.uk/ws/files/64288545/BezLopStu2017emo_igd.pdf
	•	Elite Multi-Criteria Decision Making—Pareto Front Optimization in ..., erişim tarihi Kasım 3, 2025, https://www.mdpi.com/1999-4893/17/5/206
	•	Boxplot of metrics and computational time for each MOEA (NSGA-II,... - ResearchGate, erişim tarihi Kasım 3, 2025, https://www.researchgate.net/figure/Boxplot-of-metrics-and-computational-time-for-each-MOEA-NSGA-II-NSGA-III-SPEA-II_fig3_365630615
	•	Performance Metrics Ensemble for Multiobjective Evolutionary Algorithms - Gary G. Yen, erişim tarihi Kasım 3, 2025, https://isc.okstate.edu/papers/06413195.pdf
	•	I. IntRoduction By definition, a search and optimization problem with multiple conflicting objectives resorts to a set of optima - MSU College of Engineering, erişim tarihi Kasım 3, 2025, https://www.egr.msu.edu/~kdeb/papers/k2003002.pdf
	•	Research on Multi-Objective Process Parameter Optimization Method in Hard Turning Based on an Improved NSGA-II Algorithm - MDPI, erişim tarihi Kasım 3, 2025, https://www.mdpi.com/2227-9717/12/5/950
	•	A Running Performance Metric and Termination Criterion for ..., erişim tarihi Kasım 3, 2025, https://www.egr.msu.edu/~kdeb/papers/c2020003.pdf
	•	Part IV: Analysis of Convergence - pymoo, erişim tarihi Kasım 3, 2025, https://pymoo.org/getting_started/part_4.html
	•	Diagnostic benchmarking of many-objective evolutionary algorithms ..., erişim tarihi Kasım 3, 2025, https://pure.tudelft.nl/ws/portalfiles/portal/219931645/Diagnostic_benchmarking_of_many-objective_evolutionary_algorithms_for_real-world_problems.pdf
